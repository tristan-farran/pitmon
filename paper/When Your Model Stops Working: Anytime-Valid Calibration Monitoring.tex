\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Remove paragraph indentation
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\Unif}{\mathrm{Uniform}}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{\textbf{When Your Model Stops Working:\\ Anytime-Valid Calibration Monitoring}}

\author{
  Tristan Farran\\
  \textit{MSc Computational Science, University of Amsterdam}\\
  \texttt{tristan.farran@student.uva.nl}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent Deployed machine learning models often experience calibration drift as the world changes. To address this challenge, we present PITMonitor, a sequential method detecting calibration changes. Unlike traditional calibration tests that require fixed evaluation windows, PITMonitor provides \emph{anytime-valid} false alarm control: the probability of ever raising a spurious alarm is bounded by $\alpha$, regardless of when monitoring stops. We prove Type I error control via Ville's inequality and demonstrate detection power on \texttt{river}'s FriedmanDrift dataset, while maintaining valid false alarm control. Code is available at \url{https://github.com/tristan-farran/pitmon}.
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}
%------------------------------------------------------------------------------

Probabilistic models deployed in production face a fundamental challenge: the world changes. Models encounter changes in input distributions, label frequencies, the relationship between features and targets, and so on. When these shifts occur, model calibration can degrade and simple calibration techniques may no longer suffice. 

Detecting degrading miscalibration is critical for maintaining trustworthy AI systems. A medical diagnostic model that becomes overconfident after a sensor upgrade, or a financial risk model that underestimates tail probabilities after a regime shift, can lead to consequential errors. In practice, calibration is often monitored using ad-hoc procedures such as periodic recalibration schedules, rolling-window hypothesis tests, threshold-based alerts on summary metrics, or manual inspection of residuals.

These approaches suffer from a fundamental statistical problem: \emph{they do not control the false alarm rate over continuous monitoring}. A practitioner who checks calibration daily with a $p < 0.05$ threshold will, over a year of monitoring, almost certainly observe spurious alarms even if the model remains stable. Classical hypothesis tests assume a fixed sample size determined before seeing data; continuous monitoring violates this assumption.

More principled alternatives are provided by online drift detectors, such as those implemented in the \texttt{river} library \citep{montiel2021river}. Classical detectors including DDM, EDDM, and Page-Hinkley are lightweight, easy to deploy, and effective at detecting abrupt changes, but are typically based on heuristic thresholds or fixed-sample statistical arguments and do not provide explicit long-run false alarm guarantees under continuous monitoring.

ADWIN employs sequential hypothesis testing with anytime-valid concentration bounds, offering formal control of the probability of false alarms over an unbounded stream \citep{bifet2007learning}. However, even ADWIN generally operates on generic performance statistics such as error rates and as a result, only partially solves the problem of reliable, long-term calibration monitoring.


We propose PITMonitor, a method providing \emph{anytime-valid} monitoring of PIT exchangeability with four key properties:

\begin{enumerate}
    \item \textbf{Anytime-valid false alarm control:} we prove that $\Prob(\text{ever alarm} \mid H_0) \leq \alpha$, regardless of when or why monitoring stops.
    
    \item \textbf{Change detection without static-error alarms:} PITMonitor detects and locates \emph{changes} in the PIT process. A model that is consistently miscalibrated but stable will not trigger alarms,\footnote{In many domains some amount of miscalibration is inevitable, but model degradation remains a pressing concern.} while a changing process can.
    
    \item \textbf{No baseline period required:} unlike methods requiring a ``clean'' reference distribution, PITMonitor works from the first observation by testing exchangeability of the PIT sequence.
    
    \item \textbf{Practical efficiency:} the exact algorithm runs in $O(t \log t)$ time and $O(t)$ space for $t$ observations, with a simple recursive update.
\end{enumerate}

%------------------------------------------------------------------------------
\section{Background}
%------------------------------------------------------------------------------

\subsection{Calibration and Probability Integral Transforms}

A probabilistic model outputs predicted distributions $\hat{F}$ for outcomes. The model is \emph{calibrated} if these predictions match reality: among all predictions where $\hat{F}(y) = p$, the outcome $Y \leq y$ should occur roughly $(100 \times p)$\% of the time.

The \emph{probability integral transform} (PIT) provides a universal tool for assessing calibration \citep{dawid1984statistical}.
For a continuous predictive CDF $F$ and realized outcome $y$, the PIT is $U = F(y)$.
A classical result states that if $F$ is the true distribution of $Y$, then $U \sim \Unif(0,1)$.

\begin{proposition}[PIT Uniformity]
If $Y$ has continuous CDF $F$, then $U = F(Y) \sim \Unif(0,1)$.
\end{proposition}

For discrete outcomes (e.g., classification), randomization yields a continuous PIT \citep{brockwell2007}.
Given predicted class probabilities $(\hat{p}_1, \ldots, \hat{p}_K)$ and true class $y \in \{1, \ldots, K\}$:
\begin{equation}
    U = \sum_{j=1}^{y-1} \hat{p}_j + V \cdot \hat{p}_y, \quad V \sim \Unif(0,1)
\end{equation}
Placing $U$ uniformly within the cumulative probability interval corresponding to the true class.


\begin{remark}
\textbf{Caveat for confident multi-class classification:} In $K$-class classification with highly confident predictions (i.e., when $\hat{p}_y$ is close to 1), a substantial fraction of the PIT variability can arise from the auxiliary randomization variable $V$ rather than from predictive quality. This can reduce monitoring sensitivity in this regime, as the PITs may reflect randomization noise more than model changes.
\end{remark}

Alternative PIT formulations are also valid. For instance, an empirical-CDF PIT can be constructed by computing the rank of the true class probability $\hat{p}_y$ against a reference distribution from clean calibration data: $U = \frac{\text{rank}(\hat{p}_y)}{n_{\text{ref}}} + V \cdot \Delta$, where $\Delta$ accounts for ties. Both formulations produce $U \sim \Unif(0,1)$ under calibration stability, and the theoretical guarantees of PITMonitor apply to any method satisfying this property. The empirical-CDF method is order-invariant and can be more sensitive to confidence degradation in specific domains; it is used in our experimental demonstration to achieve strong detection power on CIFAR-10-C corruption shifts while remaining theoretically justified.

Alternatively, an empirical-CDF PIT can be constructed by computing the rank of the true class probability $\hat{p}_y$ against a reference distribution from clean calibration data:
\begin{equation}
    U = \frac{\text{rank}(\hat{p}_y)}{n_{\text{ref}}} + V \cdot \Delta, \quad V \sim \Unif(0,1)
\end{equation}
where $\Delta$ accounts for ties. This order-invariant formulation is used in our experimental demonstration (see Section~\ref{sec:experiments}), as it can be more sensitive to confidence degradation in specific domains. Both methods produce $U \sim \Unif(0,1)$ under calibration stability, and the theoretical guarantees of PITMonitor apply to any method satisfying this property.

\subsection{Exchangeability}

A sequence $(X_1, X_2, \ldots)$ is \emph{exchangeable} if its joint distribution is invariant to finite permutations. Exchangeability is weaker than independence: i.i.d.\ sequences are exchangeable, but exchangeable sequences need not be independent \citep{gelman2013bda}.

\begin{remark}[Stable Miscalibration Preserves Exchangeability]
If a model is consistently miscalibrated but its calibration error distribution remains stable over time, the resulting PITs are i.i.d. from some fixed, non-uniform distribution. Since i.i.d. sequences are exchangeable, the PIT sequence remains exchangeable despite the miscalibration.
\end{remark}

This observation is central to PITMonitor's design:
\begin{itemize}
    \item \textbf{Perfect calibration:} PITs are i.i.d.\ $\Unif(0,1)$ $\Rightarrow$ exchangeable
    \item \textbf{Stable miscalibration:} PITs are i.i.d.\ from a non-uniform distribution $\Rightarrow$ still exchangeable
    \item \textbf{PIT-process change:} a change in the PIT law at some time $\tau$ $\Rightarrow$ typically not exchangeable
\end{itemize}

By testing exchangeability rather than uniformity, we avoid triggering on stable calibration error. However, non-exchangeability is broader than calibration drift: case-mix shifts, label-prior shifts, and temporal dependence can also trigger alarms even when a calibration mapping is unchanged.

\subsection{Conformal P-values from Ranks}

To sequentially test exchangeability we employ \emph{conformal p-values} \citep{vovk2005algorithmic}.

Given observations $U_1, \ldots, U_t$, define the rank of $U_t$:
\begin{equation}
    R_t = \#\{s \leq t : U_s \leq U_t\}
\end{equation}

\begin{proposition}[Rank Uniformity under Exchangeability]
If $(U_1, \ldots, U_t)$ is exchangeable, then the rank $R_t$ is uniformly distributed on $\{1, \ldots, t\}$.
\end{proposition}
\begin{proof}
By exchangeability, the joint distribution of $(U_1, \ldots, U_t)$ is invariant to permutations. For any $r \in \{1, \ldots, t\}$, the probability that any $U_i$ has rank $r$ must be equal for all $i$ by symmetry. Since exactly one element must have rank $r$ and all $t$ positions are equally likely, $\Prob(R_t = r) = 1/t$.
\end{proof}
\begin{remark}
The proof assumes continuous random variables to avoid ties. For discrete outcomes, ties can occur, but randomization (as in the randomized PIT) ensures the resulting p-values are still uniform. This subtlety is important for practical implementations.
\end{remark}

Crucially, this holds regardless of the marginal distribution of the PITs - the test is completely distribution-free.

To obtain continuous uniform p-values from the discrete uniform ranks, we randomize within ties:
\begin{equation}
    p_t = \frac{R_t - 1 + V_t}{t}, \quad V_t \sim \Unif(0,1)
\end{equation}

Under $H_0$ (exchangeability), these p-values are marginally $\Unif(0,1)$ and satisfy the sequential conformal validity conditions used by test martingales. They need not be independent across time. After a changepoint however, exchangeability breaks: new PITs come from a shifted mechanism and systematically rank higher or lower than pre-change PITs. For example, if post-change PITs tend to be smaller, they will consistently receive low ranks, causing $p_t$ to concentrate near zero rather than remaining uniform.

\subsection{E-values and Anytime-Valid Inference}

An \emph{e-value} is a nonnegative random variable $E$ satisfying $\E[E] \leq 1$ under the null hypothesis \citep{vovk2021values}. By Markov's inequality, $\Prob(E \geq 1/\alpha) \leq \alpha$, so thresholding at $1/\alpha$ provides a valid level-$\alpha$ test. Under alternatives where the null is violated, well-chosen e-values satisfy $\E[E] > 1$, providing power. The density-based construction (Section~\ref{sec:density}) achieves this adaptively without requiring a parametric specification of the alternative: when p-values concentrate due to non-exchangeability, the histogram learns the concentration pattern, yielding $\E[e] > 1$.

A key property for sequential monitoring is that e-values can be composed multiplicatively while maintaining validity under the null. If $E_1, E_2$ are conditional e-values with $\E[E_1 | \mathcal{F}_0] \leq 1$ and $\E[E_2 | \mathcal{F}_1] \leq 1$ (where $\mathcal{F}_t$ is the information available through time $t$), their product remains a valid e-value. This allows us to define a cumulative e-process:
\begin{equation}
    M_t = E_1 \times \cdots \times E_t, \quad M_t = M_{t-1} \cdot E_t
\end{equation}
Taking conditional expectations given past observations:
\begin{equation}
    \E[M_t \mid \mathcal{F}_{t-1}] = M_{t-1} \cdot \E[E_t \mid \mathcal{F}_{t-1}] \leq M_{t-1}
\end{equation}
Thus $(M_t)$ is a nonnegative supermartingale under $H_0$. This supermartingale structure ensures that the process does not grow systematically under the null: it can drift downward or remain flat. In contrast, under alternatives where $\E[E_t | \mathcal{F}_{t-1}] > 1$, the process grows exponentially, accumulating evidence of a violation.

Ville's inequality \citep{ville1939} provides the anytime-valid guarantee by bounding the probability that a nonnegative supermartingale ever exceeds a threshold:
\begin{proposition}[Ville's Inequality]
Let $(M_t)_{t \geq 1}$ be a nonnegative supermartingale with $\E[M_1] \leq 1$. Then:
\begin{equation}
    \Prob\left(\sup_{t \geq 1} M_t \geq \frac{1}{\alpha}\right) \leq \alpha
\end{equation}
\end{proposition}

 Unlike fixed-sample tests that control error only at a predetermined $n$, Ville's inequality allows monitoring to continue indefinitely while maintaining $\alpha$-level control.

%------------------------------------------------------------------------------
\section{Method}
%------------------------------------------------------------------------------

\subsection{E-values via Density Betting}
\label{sec:density}

We construct e-values from conformal p-values using a density-based betting framework \citep{shafer2021testing, grunwald2024safe}. Before observing $p_t$, we specify a density function $\hat{f}(p)$ over $[0,1]$ encoding our prior belief about where $p_t$ will concentrate. Any density function $\hat{f}(p)$ satisfying $\int_0^1 \hat{f}(p) dp = 1$ yields a valid e-value with $\E[\hat{f}(p)] = 1$ under uniformity, providing a fair bet that averages to 1 under the null while allowing high payoffs when p-values concentrate.


\begin{proposition}[Density Betting Yields Valid E-values]
\label{prop:density}
Let $\hat{f}: [0,1] \to [0,\infty)$ be any density function (i.e., $\int_0^1 \hat{f}(p) dp = 1$).
If $p \sim \Unif(0,1)$, then $e = \hat{f}(p)$ satisfies $\E[e] = 1$.
\end{proposition}


By adapting our density to observed concentration patterns, we automatically bet in the right direction: when p-values deviate from uniformity, the learned density places mass where deviations occur, and our e-value grows

PITMonitor uses a histogram density that learns from past observations:
\begin{equation}
    \hat{f}(p) = B \cdot \frac{c_b}{\sum_j c_j} \quad \text{for } p \in \text{bin } b
\end{equation}
where $c_b$ counts past p-values in bin $b$ and $B$ is the number of bins.

Under exchangeability (null), p-values scatter uniformly. With finite bins, the learned histogram spreads mass roughly evenly across bins, yielding $\E[e] \approx 1$. When exchangeability breaks, p-values cluster in certain bins; the histogram learns these concentration patterns and achieves $\E[e] > 1$, generating detection power.

We update the histogram \emph{after} computing $e_t$, ensuring $\hat{f}$ depends only on past observations. This maintains the predictability required for the supermartingale property of the e-process.

Formally, with filtration $\mathcal{F}_t = \sigma(U_1,\ldots,U_t, V_1,\ldots,V_t)$, the histogram bettor $\hat f_t$ is $\mathcal{F}_{t-1}$-measurable, and $e_t = \hat f_t(p_t)$ is therefore predictable. Under $H_0$, we use the standard conformal validity condition that $p_t$ is conditionally uniform given $\mathcal{F}_{t-1}$; hence
\begin{equation}
    \E[e_t \mid \mathcal{F}_{t-1}] = \int_0^1 \hat f_t(p)\,dp = 1.
\end{equation}
This is the fairness property required for the martingale arguments below.




\subsection{The Mixture E-process}

The key challenge is that the changepoint time $\tau$ is unknown; it could be anywhere from the start to the present. Rather than commit to a single guess, we maintain a weighted mixture over all possible changepoint times:
\begin{equation}
    M_t = \sum_{\tau=1}^{t} w_\tau \cdot M_t^{(\tau)}
\end{equation}
where $M_t^{(\tau)} = \prod_{s=\tau}^{t} e_s$ is the evidence accumulated from time $\tau$ onward, and $w_\tau = 1/(\tau(\tau+1))$ is a deterministic mixture weight, which is nonnegative and satisfies $\sum_{\tau=1}^{\infty} w_\tau = 1$. These weights define a proper mixture over candidate changepoint locations and yield an efficient recursion for the mixture e-process.

The power of this approach lies in an efficient recursion that avoids maintaining separate products for each $\tau$:

\begin{proposition}[Efficient Recursion]
The mixture e-process satisfies:
\begin{equation}
    M_t = e_t \cdot (M_{t-1} + w_t)
\end{equation}
where $w_t = 1/(t(t+1))$.
\end{proposition}
\begin{proof}
Expand the definition:
\begin{align}
M_t &= \sum_{\tau=1}^{t} w_\tau \cdot M_t^{(\tau)} \\
&= \sum_{\tau=1}^{t-1} w_\tau \cdot e_t \cdot M_{t-1}^{(\tau)} + w_t \cdot e_t \\
&= e_t \left( \sum_{\tau=1}^{t-1} w_\tau \cdot M_{t-1}^{(\tau)} + w_t \right) \\
&= e_t (M_{t-1} + w_t)
\end{align}
\end{proof}

This recursion enables $O(1)$ update of the mixture per observation (plus $O(\log t)$ for rank computation), avoiding the cost of maintaining or updating all $t$ component e-processes separately.

\subsection{Type I Error Control}

\begin{theorem}[Anytime-Valid False Alarm Control]
\label{thm:type1}
Under $H_0$ (exchangeability of PITs), the PITMonitor process $(M_t)$ satisfies:
\begin{equation}
    \Prob\left(\sup_{t \geq 1} M_t \geq \frac{1}{\alpha}\right) \leq \alpha
\end{equation}
\end{theorem}

\begin{proof}
Define the filtration $\mathcal{F}_t = \sigma(U_1,\ldots,U_t, V_1,\ldots,V_t)$. As shown above, $e_t$ is nonnegative and satisfies $\E[e_t\mid\mathcal{F}_{t-1}] = 1$ under $H_0$.

For each candidate changepoint $\tau$, define the component process
\begin{equation}
\widetilde{M}_t^{(\tau)} =
\begin{cases}
1, & t < \tau, \\
\prod_{s=\tau}^{t} e_s, & t \ge \tau.
\end{cases}
\end{equation}
Then $(\widetilde{M}_t^{(\tau)})_{t\ge 0}$ is a nonnegative martingale with respect to $(\mathcal{F}_t)$.

Now define the \emph{full} mixture
\begin{equation}
\widetilde{M}_t = \sum_{\tau=1}^{\infty} w_\tau\, \widetilde{M}_t^{(\tau)},
\qquad w_\tau = \frac{1}{\tau(\tau+1)},
\end{equation}
with $\sum_{\tau\ge 1} w_\tau = 1$. A nonnegative weighted sum of martingales is a martingale, so $(\widetilde{M}_t)$ is a nonnegative martingale (hence supermartingale) with $\E[\widetilde{M}_0]=1$.

The implemented recursion tracks the truncated mixture
\begin{equation}
M_t = \sum_{\tau=1}^{t} w_\tau \prod_{s=\tau}^{t} e_s,
\end{equation}
which satisfies $M_t = e_t(M_{t-1}+w_t)$ and $M_0=0$. Since $\widetilde{M}_t^{(\tau)}=1$ for $\tau>t$,
\begin{equation}
\widetilde{M}_t = M_t + \sum_{\tau=t+1}^{\infty} w_\tau = M_t + \frac{1}{t+1} \ge M_t.
\end{equation}
Therefore,
\begin{equation}
\left\{\sup_{t\ge 1} M_t \ge \frac{1}{\alpha}\right\}
\subseteq
\left\{\sup_{t\ge 1} \widetilde{M}_t \ge \frac{1}{\alpha}\right\}.
\end{equation}
Applying Ville's inequality to $(\widetilde{M}_t)$ yields
\begin{equation}
\Prob\left(\sup_{t\ge 1} M_t \ge \frac{1}{\alpha}\right)
\le
\Prob\left(\sup_{t\ge 1} \widetilde{M}_t \ge \frac{1}{\alpha}\right)
\le \alpha.
\end{equation}
\end{proof}

\begin{remark}[Behavior Under the Null]
Under $H_0$, the \emph{full} mixture $\widetilde{M}_t$ is a nonnegative martingale/supermartingale, while the implemented truncated statistic $M_t$ is dominated by $\widetilde{M}_t$.
Ville's inequality is therefore applied to $\widetilde{M}_t$, which still guarantees that the implemented alarm based on $M_t$ is unlikely to ever hit $1/\alpha$.
Under $H_1$, the e-values have expectation greater than 1, so $M_t$ grows exponentially and quickly crosses the threshold.
\end{remark}

\subsection{Changepoint Estimation}

After an alarm at time $T$, we estimate the changepoint by maximizing a Bayes factor.
We evaluate split points $k \in \{1, \ldots, T-1\}$, where each candidate uses the post-split segment $(p_{k+1}, \ldots, p_T)$.
For each candidate split $k$, we compare:
\begin{itemize}
    \item $H_0^{(k)}$: p-values after $k$ follow $\Unif(0,1)$
    \item $H_1^{(k)}$: p-values after $k$ follow an unknown categorical distribution
\end{itemize}

Using a Dirichlet-multinomial model with Jeffreys prior \citep{jeffreys1961} (Dirichlet with $\alpha_j = 1/2$), the log Bayes factor admits a closed form.
We select $\hat{\tau} = \argmax_k \log \text{BF}_k$.

This provides a reasonable point estimate.
For formal confidence sets, one could invert e-values testing each candidate changepoint \citep{shin2022detectors}, though this requires additional bookkeeping.

\subsection{Complete Algorithm}

Having introduced all components, we now present the complete PITMonitor algorithm:

\begin{algorithm}
\caption{PITMonitor}
\label{alg:pitmonitor}
\begin{algorithmic}[1]
\Require Significance level $\alpha$, number of bins $B$
\State Initialize: $M_0 \gets 0$, histogram counts $c_1, \ldots, c_B \gets 1$ \Comment{Laplace prior}
\For{$t = 1, 2, \ldots$}
    \State Observe PIT $U_t \in [0,1]$
    \State Insert $U_t$ into sorted list; compute rank $R_t$
    \State Sample $V_t \sim \Unif(0,1)$
    \State $p_t \gets (R_t - 1 + V_t) / t$ \Comment{Conformal p-value}
    \State $b \gets \lfloor p_t \cdot B \rfloor + 1$ \Comment{Histogram bin index}
    \State $e_t \gets B \cdot c_b / \sum_{j=1}^B c_j$ \Comment{E-value from density}
    \State $c_b \gets c_b + 1$ \Comment{Update histogram after computing $e_t$}
    \State $w_t \gets 1 / (t \cdot (t+1))$ \Comment{Deterministic mixture weight}
    \State $M_t \gets e_t \cdot (M_{t-1} + w_t)$ \Comment{Mixture e-process}
    \If{$M_t \geq 1/\alpha$}
        \State \textbf{return} ALARM at time $t$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}
%------------------------------------------------------------------------------

We evaluate PITMonitor on two benchmark suites that isolate complementary failure modes and baseline tradeoffs: CIFAR-10/10-C corruption shifts and a controlled delivery-demo stream with tunable shift magnitude.

\subsection{Secondary Robustness Benchmark Setup (CIFAR)}

The setup in this subsection corresponds to the CIFAR corruption benchmark, which we treat as a secondary robustness stress test rather than the primary real-drift benchmark.

\paragraph{Dataset.}
We use CIFAR-10 \citep{krizhevsky2009learning} for training and clean test data, and CIFAR-10-C \citep{hendrycks2019benchmarking} for corrupted test data.
CIFAR-10-C contains 19 corruption types at 5 severity levels; we use Gaussian noise as a representative corruption.

\paragraph{Model.}
We train an MLP classifier with hidden layers $(64, 32, 16)$, ReLU activations, and Adam optimizer on 15,000 CIFAR-10 training images.

\paragraph{Monitoring Protocol.}
Each trial consists of:
\begin{itemize}
    \item \textbf{Stable phase}: $n_{\text{stable}} = 300$ predictions on clean CIFAR-10 test images
    \item \textbf{Shifted phase}: $n_{\text{shifted}} = 300$ predictions on CIFAR-10-C images
\end{itemize}
The true changepoint is at $t = 301$.
We compute randomized classification PITs and run PITMonitor with $\alpha = 0.05$, $B = 10$ bins.

\paragraph{Metrics.}
\begin{itemize}
    \item \textbf{False positive rate (FPR)}: Proportion of $H_0$ trials with alarm before $t = 301$ (Wilson score 95\% confidence interval)
    \item \textbf{True positive rate (TPR)}: Proportion of $H_1$ trials with alarm after $t = 301$ (Wilson score 95\% confidence interval)
    \item \textbf{Detection delay}: Observations from changepoint to alarm (for true positives)
\end{itemize}

We run 1,000 trials per condition and report Wilson score 95\% confidence intervals for both FPR and TPR.

\subsection{Results}

The numerical results below correspond to the currently implemented CIFAR and delivery benchmark suites.

\paragraph{Type I Error Control.}
Table~\ref{tab:h0} reports FPR under $H_0$ (clean $\to$ clean, no actual shift).
The observed FPR is well below the nominal $\alpha = 0.05$, empirically confirming Theorem~\ref{thm:type1}.

\begin{table}[ht]
\centering
\caption{False positive rate under $H_0$ (no distribution shift). PITMonitor controls FPR below the nominal $\alpha = 0.05$ level as guaranteed by Ville's inequality.}
\label{tab:h0}
\begin{tabular}{@{}lcc@{}}
\toprule
Condition & FPR & 95\% CI \\
\midrule
Clean $\to$ Clean & 2.0\% & [0.4\%, 7.0\%] \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Detection Power.}
Table~\ref{tab:power} reports TPR and median detection delay across corruption severities.
Detection power increases monotonically with severity, reaching near-perfect detection at severity 5.
Detection delay decreases as shift magnitude increases---larger shifts produce stronger evidence per observation.

\begin{table}[ht]
\centering
\caption{Detection performance across CIFAR-10-C Gaussian noise severities. Higher severity corresponds to stronger corruption and easier detection.}
\label{tab:power}
\begin{tabular}{@{}cccc@{}}
\toprule
Severity & TPR & 95\% CI & Median Delay \\
\midrule
1 & 45\% & [35\%, 55\%] & 142 \\
2 & 68\% & [58\%, 77\%] & 98 \\
3 & 84\% & [75\%, 90\%] & 67 \\
4 & 93\% & [86\%, 97\%] & 48 \\
5 & 98\% & [93\%, 100\%] & 32 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Part 3 Baseline Comparisons (CIFAR).}
Following the CIFAR demo notebook's Part 3 protocol, we compare PITMonitor to four standard stream-drift detectors (DDM, EDDM, ADWIN, KSWIN) on the same clean $\to$ corrupted stream design.
Each method is evaluated over 1{,}000 Monte Carlo trials per severity with identical observation budgets and $\alpha=0.05$.
Table~\ref{tab:cifar_part3_compare} reports the exact benchmark outputs used in the demo.

\begin{table}[ht]
\centering
\caption{CIFAR Part 3 comparison (clean $\to$ CIFAR-10-C Gaussian noise) using 1{,}000 trials per severity. 95\% Wilson score confidence intervals are reported for FPR and TPR.}
\label{tab:cifar_part3_compare}
\begin{tabular}{@{}ccccccc@{}}
\toprule
Severity & Method & FPR & FPR CI & TPR & TPR CI & Median Delay \\
\midrule
1 & PITMonitor & 3.5\% & [2.0\%, 5.9\%] & 96.5\% & [94.1\%, 98.0\%] & 107.0 \\
1 & DDM & 4.8\% & [2.8\%, 7.7\%] & 92.4\% & [89.2\%, 94.7\%] & 71.0 \\
1 & EDDM & 85.2\% & [81.7\%, 88.2\%] & 14.8\% & [11.8\%, 18.3\%] & 46.0 \\
1 & ADWIN & 0.0\% & [0.0\%, 0.7\%] & 100.0\% & [99.3\%, 100.0\%] & 116.0 \\
1 & KSWIN & 4.8\% & [2.8\%, 7.7\%] & 45.0\% & [40.7\%, 49.4\%] & 29.0 \\
\midrule
3 & PITMonitor & 4.2\% & [2.4\%, 7.0\%] & 95.7\% & [93.1\%, 97.5\%] & 108.0 \\
3 & DDM & 3.5\% & [1.9\%, 6.2\%] & 93.3\% & [90.3\%, 95.5\%] & 72.0 \\
3 & EDDM & 84.0\% & [80.3\%, 87.2\%] & 16.0\% & [12.8\%, 19.7\%] & 52.5 \\
3 & ADWIN & 0.0\% & [0.0\%, 0.7\%] & 100.0\% & [99.3\%, 100.0\%] & 116.0 \\
3 & KSWIN & 5.8\% & [3.6\%, 9.1\%] & 41.6\% & [37.4\%, 46.0\%] & 29.5 \\
\midrule
5 & PITMonitor & 3.3\% & [1.8\%, 5.6\%] & 96.7\% & [94.4\%, 98.2\%] & 106.0 \\
5 & DDM & 3.7\% & [2.1\%, 6.4\%] & 94.3\% & [91.5\%, 96.4\%] & 71.0 \\
5 & EDDM & 85.2\% & [81.7\%, 88.2\%] & 14.8\% & [11.8\%, 18.3\%] & 47.5 \\
5 & ADWIN & 0.0\% & [0.0\%, 0.7\%] & 100.0\% & [99.3\%, 100.0\%] & 116.0 \\
5 & KSWIN & 4.0\% & [2.2\%, 7.0\%] & 47.3\% & [42.9\%, 51.7\%] & 29.0 \\
\bottomrule
\end{tabular}
\end{table}

These CIFAR comparisons show that PITMonitor maintains FPR near the target level while achieving high TPR across severities.
DDM is similarly powerful but with somewhat higher false alarms, ADWIN is extremely conservative (zero FPR) with later alarms, KSWIN is fast but materially less sensitive at these severities, and EDDM is unstable in this setting.

The delivery benchmark reinforces the same tradeoff profile: PITMonitor attains high power with controlled false alarms, DDM and KSWIN often detect earlier but can incur higher false positives depending on regime, ADWIN is conservative and often delayed, and EDDM is dominated by excessive false alarms.

\paragraph{Qualitative Behavior.}
Figure~\ref{fig:detection} illustrates a typical monitoring trace.
During the stable phase, the monitored statistic typically stays small under $H_0$ because it is bounded by a valid supermartingale.
After the shift at $t = 301$, evidence accumulates exponentially as the conformal p-values concentrate, quickly crossing the alarm threshold.
The estimated changepoint closely tracks the true shift location.

\paragraph{Scope of What Is Detected.}
The empirical shifts used here alter calibration and therefore PIT exchangeability, but this design does not isolate all possible non-exchangeability sources.
In particular, case-mix shifts that preserve a reliability curve can still change PIT marginals and trigger alarms.
Thus the experiments should be interpreted as exchangeability-change detection evidence, with calibration drift as an important but not exclusive mechanism.

\begin{figure}[ht]
\centering
\fbox{\parbox{0.9\textwidth}{\centering\vspace{2em}[Detection trace figure: 4-panel plot showing (1) confidence vs accuracy over time, (2) PIT stream with rolling mean, (3) log-evidence with threshold and alarm, (4) pre/post PIT histograms]\vspace{2em}}}
\caption{PITMonitor detection on CIFAR-10 $\to$ CIFAR-10-C (Gaussian noise, severity 3). Top left: model confidence and accuracy degrade after the shift. Top right: PITs shift from roughly uniform to concentrated. Bottom left: e-process grows exponentially post-shift, crossing the threshold. Bottom right: PIT histograms show the distributional change.}
\label{fig:detection}
\end{figure}

%------------------------------------------------------------------------------
\section{Discussion}
%------------------------------------------------------------------------------

\paragraph{When to Use PITMonitor.}
PITMonitor is designed for continuous monitoring of deployed probabilistic models where:
\begin{itemize}
    \item False alarms have real costs (unnecessary retraining, alert fatigue, loss of trust)
    \item The monitoring horizon is indefinite or stopping is data-dependent
    \item Calibration \emph{drift}---not static miscalibration---is the concern
\end{itemize}

For one-time calibration assessment (``is this model calibrated?''), standard methods like reliability diagrams or Expected Calibration Error suffice.
PITMonitor addresses the harder problem of continuous monitoring with statistical guarantees.

\paragraph{Limitations.}

\textit{Exchangeability assumption.}
PITMonitor tests exchangeability of PITs.
If pre-change PITs exhibit temporal dependence (e.g., autocorrelated predictions from a time series model), exchangeability may not hold exactly under $H_0$.
Mild violations appear tolerable empirically, but strongly dependent streams may require extensions such as block-exchangeability, block permutations, or explicitly calibrated nulls under mixing conditions.

\textit{Operational memory and sensitivity decay.}
The exact rank computation stores all historical PITs and uses cumulative histogram counts, giving $O(t)$ memory and potential inertia after long stable periods.
Practical deployments may use windowing, exponential forgetting, or quantile sketches for bounded memory.
These modifications generally trade theoretical exactness for responsiveness and engineering feasibility; formal guarantees should then be re-derived for the chosen approximation.

\textit{Power for small shifts.}
Subtle calibration changes require many observations to detect.
At severity 1 in our experiments, TPR is 45\%---substantial but not overwhelming.
This reflects a fundamental tradeoff: strong FPR control (anytime-valid, at all stopping times) implies slower detection of small shifts.
Users can increase power by accepting a larger $\alpha$ or waiting longer before acting on alarms.

\textit{Classification randomization.}
The randomized PIT for classification injects noise, especially for binary outcomes or low-confidence predictions.
With many classes and confident predictions (as in CIFAR-10), this is negligible.

\paragraph{Classification with confident predictions.}
The randomized PIT construction works cleanly and well for continuous outcomes. However, in multi-class classification settings with highly confident predictions (i.e., when the predicted probability for the true class is close to 1), the randomization noise introduced by $V$ can dominate the PIT signal. This leads to a low signal-to-noise ratio for change detection, as much of the PIT variability is due to randomization rather than predictive quality. Importantly, this is a limitation of the standard classification PIT itself, not of the monitoring or betting procedure. In such regimes, ECDF-based or rank-based PIT alternatives may provide greater sensitivity and are recommended for practitioners seeking improved detection power.

\textit{Classification randomization.}
The randomized PIT for classification injects noise, especially for binary outcomes or low-confidence predictions. With many classes and confident predictions (as in CIFAR-10), this is negligible for most practical purposes, but see the above paragraph for limitations in the highly confident regime.

\textit{Changepoint localization.}
The Bayes factor estimate provides a reasonable point estimate but lacks formal coverage guarantees.
Confidence sets could be constructed by inverting e-values for each candidate changepoint, at the cost of additional computation.

\paragraph{Practical Recommendations.}
\begin{itemize}
    \item Set $\alpha$ based on tolerance for false alarms over the deployment horizon. For safety-critical systems, $\alpha = 0.01$ may be appropriate; for exploratory monitoring, $\alpha = 0.10$ allows faster detection.
    \item Use $B = 10$ histogram bins as a default. More bins accelerate adaptation but increase variance; fewer bins are more stable but slower to learn.
    \item After an alarm, use the changepoint estimate to identify when drift began, then investigate root causes before retraining.
    \item Consider running PITMonitor in parallel with a lower $\alpha$ (e.g., 0.01) for high-confidence alerts and a higher $\alpha$ (e.g., 0.10) for early warnings.
\end{itemize}

\paragraph{Baseline and Negative-Control Evaluation.}
The Part 3 CIFAR and delivery benchmarks provide a practical ``price of validity'' view by contrasting PITMonitor with DDM, EDDM, ADWIN, and KSWIN under matched streams.
An important extension remains an explicit negative-control scenario such as \emph{Static Poor $\to$ Static Poor}, where calibration is bad but stationary; PITMonitor should remain mostly silent there, while generic drift detectors may still alarm.

%------------------------------------------------------------------------------
\section{Related Work}
%------------------------------------------------------------------------------

\subsubsection*{Calibration Assessment}
Classical calibration metrics include Expected Calibration Error \citep{naeini2015obtaining}, reliability diagrams \citep{degroot1983comparison}, and proper scoring rules \citep{gneiting2007strictly}.
These provide point-in-time assessments but do not address sequential monitoring with false alarm control.
PITs have been used for forecast evaluation in econometrics \citep{diebold1998evaluating} and weather prediction \citep{gneiting2014probabilistic}.

\subsubsection*{Distribution Shift Detection}
Methods for detecting covariate shift include two-sample tests \citep{rabanser2019failing}, domain classifiers \citep{lipton2018detecting}, and conformal approaches \citep{podkopaev2021distribution}.
These typically focus on input distribution changes rather than calibration specifically.
Our work focuses on the \emph{output} side: detecting when predicted probabilities no longer match outcome frequencies.

\subsubsection*{Sequential Calibration Testing}
\citet{arnold2023sequentially} proposed e-values for testing forecast calibration, focusing on whether PITs are uniform.
Our work differs in two ways: (1) we test exchangeability rather than uniformity, enabling insensitivity to i.i.d. stable miscalibration while remaining sensitive to broader non-exchangeability; (2) we use the mixture e-detector framework for changepoint detection rather than simple hypothesis testing.

\subsubsection*{E-values and Anytime-Valid Inference}
The e-value framework has seen rapid development \citep{vovk2021values, ramdas2023game, grunwald2024safe}.
Applications include A/B testing \citep{johari2022always}, clinical trials \citep{wassmer2016group}, and conformal prediction \citep{vovk2005algorithmic}.
The e-detector framework for changepoint detection was introduced by \citet{shin2022detectors}, providing the theoretical foundation for our mixture e-process.

\subsubsection*{Changepoint Detection}
Classical methods include CUSUM \citep{page1954continuous} and Shiryaev-Roberts procedures \citep{shiryaev1963, pollak1985optimal}.
These typically assume known pre- and post-change distributions.
The e-detector approach provides nonparametric changepoint detection with finite-sample guarantees.

%------------------------------------------------------------------------------
\section{Conclusion}
%------------------------------------------------------------------------------

We presented PITMonitor, a method for detecting exchangeability violations in PIT streams with anytime-valid false alarm guarantees.
By testing exchangeability of probability integral transforms using a mixture e-process, PITMonitor enables continuous monitoring without inflating Type I error, regardless of when or why monitoring stops.

The method addresses a practical gap in ML operations: statistically principled continuous monitoring that is silent on static i.i.d. miscalibration yet sensitive to process changes.
Experiments on CIFAR-10 to CIFAR-10-C shifts demonstrate effective detection across corruption severities while maintaining valid error control.

Future work includes extensions to temporally dependent predictions, multivariate outputs (monitoring multiple models jointly), and integration with adaptive recalibration triggered by detected drift.

\paragraph{Code Availability.}
PITMonitor is available at \url{https://github.com/tristan-farran/pitmon}.

%------------------------------------------------------------------------------
% References
%------------------------------------------------------------------------------

\bibliographystyle{plainnat}

\bibliography{pitmon_references}
\end{document}

% Correct Shafer reference year: Statistical Science 2011, not 2021
https://iclr.cc/
https://icml.cc/
https://imstat.org/journals-and-publications/annals-of-applied-statistics/
https://jmlr.org/
https://neurips.cc/
https://virtual.aistats.org/
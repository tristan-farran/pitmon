\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\Unif}{\mathrm{Uniform}}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{\textbf{Monitoring Calibration Drift in Machine Learning Models with Anytime-Valid Guarantees}}

\author{
  Tristan Farran\\
  \textit{MSc Computational Science, University of Amsterdam}\\
  % \texttt{email@university.edu}
}

\date{\today}

\begin{document}

\begin{document}

\maketitle

\begin{abstract}
Deployed machine learning models often experience calibration drift as the data distribution shifts over time. 
We present PITMonitor, a sequential method for detecting when a probabilistic model's calibration changes. 
Unlike traditional calibration tests that require fixed evaluation windows, PITMonitor provides \emph{anytime-valid} false alarm control: the probability of ever raising a spurious alarm is bounded by $\alpha$, regardless of when monitoring stops or what stopping rule is used.
The method works by testing exchangeability of probability integral transforms (PITs) using a mixture e-process. 
Under stable calibration---even if imperfect---PITs are exchangeable and no alarm fires.
When calibration changes, exchangeability breaks and evidence accumulates.
We prove Type I error control via Ville's inequality and demonstrate detection power on CIFAR-10 to CIFAR-10-C distribution shifts, achieving high detection rates while maintaining valid false alarm control across varying corruption severities.
Code is available at \url{https://github.com/tristan-farran/pitmon}.

\vspace{0.5em}
\noindent\textbf{Keywords:}probability integral transform, calibration monitoring, e-processes, sequential hypothesis testing, exchangeability, distribution shift, model reliability
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}
%------------------------------------------------------------------------------

Machine learning models deployed in production face a fundamental challenge: the world changes. 
Models trained on historical data encounter distribution shifts---changes in input distributions, label frequencies, or the relationship between features and targets. 
When these shifts occur, model calibration often degrades: predicted probabilities no longer reflect true outcome frequencies \citep{guo2017calibration, minderer2021revisiting}.

Detecting calibration drift is critical for maintaining trustworthy AI systems. 
A medical diagnostic model that becomes overconfident after a sensor upgrade, or a financial risk model that underestimates tail probabilities after a market regime change, can lead to consequential errors. 
Yet practitioners often rely on ad-hoc monitoring: periodic recalibration schedules, threshold-based alerts on rolling metrics, or manual inspection of reliability diagrams.

These approaches suffer from a fundamental statistical problem: \emph{they do not control the false alarm rate over continuous monitoring}. 
A practitioner who checks calibration daily with a $p < 0.05$ threshold will, over a year of monitoring, almost certainly observe spurious alarms even if the model remains stable. 
Classical hypothesis tests assume a fixed sample size determined before seeing data; continuous monitoring violates this assumption.

We propose PITMonitor, a method providing \emph{anytime-valid} calibration monitoring with four key properties:

\begin{enumerate}
    \item \textbf{Anytime-valid false alarm control.} We prove that $\Prob(\text{ever alarm} \mid H_0) \leq \alpha$, regardless of when or why monitoring stops. This guarantee holds even under adaptive, data-dependent stopping rules.
    
    \item \textbf{Change detection, not miscalibration detection.} PITMonitor detects \emph{changes} in calibration, not static miscalibration. A model that is consistently overconfident will not trigger alarms; only shifts in calibration behavior are flagged.
    
    \item \textbf{No baseline period required.} Unlike methods requiring a ``clean'' reference distribution, PITMonitor works from the first observation by testing exchangeability of the PIT sequence.
    
    \item \textbf{Practical efficiency.} The algorithm runs in $O(t \log t)$ time and $O(t)$ space for $t$ observations, with a simple recursive update.
\end{enumerate}

%------------------------------------------------------------------------------
\section{Background}
%------------------------------------------------------------------------------

\subsection{Calibration and Probability Integral Transforms}

A probabilistic model outputs predicted distributions $\hat{F}$ for outcomes. 
The model is \emph{calibrated} if these predictions match reality: among all predictions where $\hat{F}(y) = 0.7$, the outcome $Y \leq y$ should occur roughly 70\% of the time.

The \emph{probability integral transform} (PIT) provides a universal tool for assessing calibration \citep{dawid1984statistical, diebold1998evaluating}.
For a continuous predictive CDF $F$ and realized outcome $y$, the PIT is $U = F(y)$.
A classical result states that if $F$ is the true distribution of $Y$, then $U \sim \Unif(0,1)$.

\begin{proposition}[PIT Uniformity]
If $Y$ has continuous CDF $F$, then $U = F(Y) \sim \Unif(0,1)$.
\end{proposition}
\begin{proof}
$\Prob(U \leq u) = \Prob(F(Y) \leq u) = \Prob(Y \leq F^{-1}(u)) = F(F^{-1}(u)) = u$.
\end{proof}

The intuition is geometric: the CDF maps outcomes to their quantile positions, and quantile positions are uniform by definition.
If a model is calibrated, its predicted CDF equals the true CDF, so PITs are uniform.
Miscalibration manifests as non-uniform PITs: overconfident models produce U-shaped histograms; underconfident models produce peaked histograms.

For discrete outcomes (e.g., classification), randomization yields a continuous PIT \citep{czado2009predictive}.
Given predicted class probabilities $(\hat{p}_1, \ldots, \hat{p}_K)$ and true class $y \in \{1, \ldots, K\}$:
\begin{equation}
    U = \sum_{j=1}^{y-1} \hat{p}_j + V \cdot \hat{p}_y, \quad V \sim \Unif(0,1)
\end{equation}
This places $U$ uniformly within the cumulative probability interval corresponding to the true class.

\subsection{Exchangeability: The Key Insight}

A sequence $(X_1, X_2, \ldots)$ is \emph{exchangeable} if its joint distribution is invariant to finite permutations.
Exchangeability is weaker than independence: i.i.d.\ sequences are exchangeable, but exchangeable sequences need not be independent (e.g., draws from a randomly chosen urn).

\begin{remark}[Stable Miscalibration Preserves Exchangeability]
Suppose a model is miscalibrated but \emph{consistently} miscalibrated---the same way at every time step.
Then each PIT $U_t$ is drawn from the same (possibly non-uniform) distribution, independently across time.
Same distribution plus independence equals i.i.d., which implies exchangeability.
\end{remark}

This observation is central to PITMonitor's design:
\begin{itemize}
    \item \textbf{Perfect calibration:} PITs are i.i.d.\ $\Unif(0,1)$ $\Rightarrow$ exchangeable
    \item \textbf{Stable miscalibration:} PITs are i.i.d.\ from some fixed non-uniform distribution $\Rightarrow$ still exchangeable
    \item \textbf{Calibration drift:} PIT distribution changes at some time $\tau$ $\Rightarrow$ \textbf{not exchangeable}
\end{itemize}

By testing exchangeability rather than uniformity, we detect \emph{changes} without triggering on stable (if imperfect) calibration.

\subsection{Conformal P-values from Ranks}

How do we test exchangeability sequentially?
The key tool is \emph{conformal p-values} \citep{vovk2005algorithmic}.

Given observations $U_1, \ldots, U_t$, define the rank of $U_t$:
\begin{equation}
    R_t = \#\{s \leq t : U_s \leq U_t\}
\end{equation}

\begin{proposition}[Rank Uniformity under Exchangeability]
If $(U_1, \ldots, U_t)$ is exchangeable, then $R_t$ is uniformly distributed on $\{1, \ldots, t\}$.
\end{proposition}

The proof follows from symmetry: under exchangeability, all orderings are equally likely, so $U_t$ is equally likely to be the smallest, second smallest, ..., or largest.

\textbf{Crucially, this holds regardless of the marginal distribution of the $U_t$'s.}
Even if PITs are non-uniform (stable miscalibration), their ranks are uniform under exchangeability.
This makes the test distribution-free.

To obtain continuous p-values, we randomize within ties:
\begin{equation}
    p_t = \frac{R_t - 1 + V_t}{t}, \quad V_t \sim \Unif(0,1)
\end{equation}
Under exchangeability, $p_t \sim \Unif(0,1)$.

After a changepoint, new PITs systematically rank higher or lower than old ones (they come from a different distribution), so $p_t$ concentrates away from uniformity.

\subsection{E-values and Anytime-Valid Inference}

An \emph{e-value} is a nonnegative random variable $E$ satisfying $\E[E] \leq 1$ under the null hypothesis \citep{vovk2021values, grunwald2024safe}.
E-values measure evidence against $H_0$: by Markov's inequality, $\Prob(E \geq 1/\alpha) \leq \alpha$.

The power of e-values lies in their composition.
If $E_1, E_2$ are independent e-values, their product $E_1 \cdot E_2$ is also an e-value:
\begin{equation}
    \E[E_1 \cdot E_2] = \E[E_1] \cdot \E[E_2] \leq 1
\end{equation}

This multiplicative composition enables sequential testing.
If we accumulate e-values $M_t = E_1 \times \cdots \times E_t$, the process $(M_t)$ is a supermartingale under $H_0$ (it doesn't grow on average).

\textbf{Ville's inequality} (1939) provides the anytime-valid guarantee:
\begin{theorem}[Ville's Inequality]
Let $(M_t)_{t \geq 1}$ be a nonnegative supermartingale with $\E[M_1] \leq 1$. Then:
\begin{equation}
    \Prob\left(\sup_{t \geq 1} M_t \geq \frac{1}{\alpha}\right) \leq \alpha
\end{equation}
\end{theorem}

This bounds the probability that the process \emph{ever} exceeds $1/\alpha$---not just at a fixed sample size, but at any stopping time, including data-dependent ones.
This is the foundation of anytime-valid inference.

%------------------------------------------------------------------------------
\section{Method}
%------------------------------------------------------------------------------

\subsection{Algorithm Overview}

PITMonitor maintains three components:
\begin{enumerate}
    \item A sorted list of observed PITs for computing ranks
    \item A histogram of conformal p-values for density estimation
    \item A mixture e-process accumulating evidence against exchangeability
\end{enumerate}

At each time step, we:
\begin{enumerate}
    \item Insert the new PIT and compute its conformal p-value
    \item Compute an e-value by evaluating a density estimate at the p-value
    \item Update the mixture e-process
    \item Alarm if the process exceeds $1/\alpha$
\end{enumerate}

\begin{algorithm}[t]
\caption{PITMonitor}
\label{alg:pitmonitor}
\begin{algorithmic}[1]
\Require Significance level $\alpha$, number of bins $B$
\State Initialize: $M_0 \gets 0$, histogram counts $c_1, \ldots, c_B \gets 1$ \Comment{Laplace prior}
\For{$t = 1, 2, \ldots$}
    \State Observe PIT $U_t \in [0,1]$
    \State Insert $U_t$ into sorted list; compute rank $R_t$
    \State Sample $V_t \sim \Unif(0,1)$
    \State $p_t \gets (R_t - 1 + V_t) / t$ \Comment{Conformal p-value}
    \State $b \gets \lfloor p_t \cdot B \rfloor + 1$ \Comment{Histogram bin index}
    \State $e_t \gets B \cdot c_b / \sum_{j=1}^B c_j$ \Comment{E-value from density}
    \State $c_b \gets c_b + 1$ \Comment{Update histogram after computing $e_t$}
    \State $w_t \gets 1 / ((t-1) \cdot t)$ \Comment{Shiryaev-Roberts weight}
    \State $M_t \gets e_t \cdot (M_{t-1} + w_t)$ \Comment{Mixture e-process}
    \If{$M_t \geq 1/\alpha$}
        \State \textbf{return} ALARM at time $t$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{E-values via Density Betting}

The key step is constructing e-values from conformal p-values.
We use a betting interpretation \citep{shafer2021testing, grunwald2024safe}.

Think of constructing an e-value as placing bets on where $p_t$ will land.
Before observing $p_t$, we specify a density function $\hat{f}(p)$ over $[0,1]$.
Our payout is $e_t = \hat{f}(p_t)$---we win more if $p_t$ lands where we bet heavily.

\begin{proposition}[Density Betting Yields Valid E-values]
\label{prop:density}
Let $\hat{f}: [0,1] \to [0,\infty)$ be any density function (i.e., $\int_0^1 \hat{f}(p) dp = 1$).
If $p \sim \Unif(0,1)$, then $e = \hat{f}(p)$ satisfies $\E[e] = 1$.
\end{proposition}
\begin{proof}
$\E[e] = \int_0^1 \hat{f}(p) \cdot 1 \, dp = 1$.
\end{proof}

Under the null (uniform $p$), any density integrates to 1, so we can't win on average.
Under the alternative, if $p$ concentrates and our density estimate $\hat{f}$ concentrates in the same region, we achieve $\E[e] > 1$.

PITMonitor uses a histogram density:
\begin{equation}
    \hat{f}(p) = B \cdot \frac{c_b}{\sum_j c_j} \quad \text{for } p \in \text{bin } b
\end{equation}
where $c_b$ is the count of past p-values in bin $b$.
The histogram learns where p-values concentrate and bets accordingly.

\textbf{Predictability requirement:} We update the histogram \emph{after} computing $e_t$, so $\hat{f}$ depends only on $p_1, \ldots, p_{t-1}$.
This ensures the betting strategy is predictable (measurable with respect to past observations), preserving the supermartingale property.

\subsection{The Mixture E-process}

We don't know when a changepoint occurred.
If we knew it happened at time $\tau$, we would accumulate evidence starting from $\tau$:
\begin{equation}
    M_t^{(\tau)} = \prod_{s=\tau}^{t} e_s
\end{equation}

Since $\tau$ is unknown, we maintain a mixture over all possible start times:
\begin{equation}
    M_t = \sum_{\tau=1}^{t} w_\tau \cdot M_t^{(\tau)}
\end{equation}
where $w_\tau$ is a prior weight on the changepoint occurring at time $\tau$.

The \textbf{Shiryaev-Roberts} weights $w_\tau = 1/(\tau(\tau+1))$ have several desirable properties \citep{shiryaev1963, pollak1985optimal}:
\begin{enumerate}
    \item They form a valid prior: $\sum_{\tau=1}^{\infty} \frac{1}{\tau(\tau+1)} = 1$ (telescoping sum)
    \item They give reasonable weight to both early and late changepoints ($w_\tau \approx 1/\tau^2$)
    \item They achieve near-optimal detection delay among procedures with the same false alarm rate
\end{enumerate}

\begin{proposition}[Efficient Recursion]
The mixture e-process satisfies:
\begin{equation}
    M_t = e_t \cdot (M_{t-1} + w_t)
\end{equation}
where $w_t = 1/((t-1)t)$.
\end{proposition}
\begin{proof}
Expand the definition:
\begin{align}
M_t &= \sum_{\tau=1}^{t} w_\tau \cdot M_t^{(\tau)} \\
&= \sum_{\tau=1}^{t-1} w_\tau \cdot e_t \cdot M_{t-1}^{(\tau)} + w_t \cdot e_t \\
&= e_t \left( \sum_{\tau=1}^{t-1} w_\tau \cdot M_{t-1}^{(\tau)} + w_t \right) \\
&= e_t (M_{t-1} + w_t)
\end{align}
\end{proof}

This gives an $O(1)$ update per observation (plus $O(\log t)$ for rank computation via binary search).

\subsection{Type I Error Control}

\begin{theorem}[Anytime-Valid False Alarm Control]
\label{thm:type1}
Under $H_0$ (exchangeability of PITs), the PITMonitor process $(M_t)$ satisfies:
\begin{equation}
    \Prob\left(\sup_{t \geq 1} M_t \geq \frac{1}{\alpha}\right) \leq \alpha
\end{equation}
\end{theorem}

\begin{proof}
Under exchangeability, conformal p-values $p_1, p_2, \ldots$ are i.i.d.\ $\Unif(0,1)$.

By Proposition~\ref{prop:density}, each e-value satisfies $\E[e_t | \mathcal{F}_{t-1}] = 1$, where $\mathcal{F}_{t-1}$ is the filtration generated by observations up to time $t-1$.

Each component e-process $M_t^{(\tau)} = \prod_{s=\tau}^{t} e_s$ is a nonnegative martingale starting at 1.

The mixture $M_t = \sum_{\tau} w_\tau M_t^{(\tau)}$ is a weighted sum of martingales with weights summing to 1, hence a supermartingale with $\E[M_1] \leq 1$.

Ville's inequality gives the result.
\end{proof}

\begin{remark}[Behavior Under the Null]
Under $H_0$, the e-process $M_t$ is a supermartingale---it may shrink, stay flat, or occasionally spike, but it doesn't grow systematically.
Ville's inequality guarantees it's unlikely to ever hit the threshold $1/\alpha$.
Under $H_1$, the e-values have expectation greater than 1, so $M_t$ grows exponentially and quickly crosses the threshold.
\end{remark}

\subsection{Changepoint Estimation}

After an alarm at time $T$, we estimate the changepoint by maximizing a Bayes factor.
For each candidate split $k$, we compare:
\begin{itemize}
    \item $H_0^{(k)}$: p-values after $k$ follow $\Unif(0,1)$
    \item $H_1^{(k)}$: p-values after $k$ follow an unknown categorical distribution
\end{itemize}

Using a Dirichlet-multinomial model with Jeffreys prior (Dirichlet with $\alpha_j = 1/2$), the log Bayes factor admits a closed form.
We select $\hat{\tau} = \argmax_k \log \text{BF}_k$.

This provides a reasonable point estimate.
For formal confidence sets, one could invert e-values testing each candidate changepoint \citep{shin2022detectors}, though this requires additional bookkeeping.

%------------------------------------------------------------------------------
\section{Experiments}
%------------------------------------------------------------------------------

We evaluate PITMonitor on detecting calibration drift when a neural network encounters distribution shift.

\subsection{Experimental Setup}

\paragraph{Dataset.}
We use CIFAR-10 \citep{krizhevsky2009learning} for training and clean test data, and CIFAR-10-C \citep{hendrycks2019benchmarking} for corrupted test data.
CIFAR-10-C contains 19 corruption types at 5 severity levels; we use Gaussian noise as a representative corruption.

\paragraph{Model.}
We train an MLP classifier with hidden layers $(64, 32, 16)$, ReLU activations, and Adam optimizer on 15,000 CIFAR-10 training images.

\paragraph{Monitoring Protocol.}
Each trial consists of:
\begin{itemize}
    \item \textbf{Stable phase}: $n_{\text{stable}} = 300$ predictions on clean CIFAR-10 test images
    \item \textbf{Shifted phase}: $n_{\text{shifted}} = 300$ predictions on CIFAR-10-C images
\end{itemize}
The true changepoint is at $t = 301$.
We compute randomized classification PITs and run PITMonitor with $\alpha = 0.05$, $B = 10$ bins.

\paragraph{Metrics.}
\begin{itemize}
    \item \textbf{False positive rate (FPR)}: Proportion of $H_0$ trials with alarm before $t = 301$
    \item \textbf{True positive rate (TPR)}: Proportion of $H_1$ trials with alarm after $t = 301$
    \item \textbf{Detection delay}: Observations from changepoint to alarm (for true positives)
\end{itemize}

We run 100 trials per condition and report Wilson score 95\% confidence intervals.

\subsection{Results}

\paragraph{Type I Error Control.}
Table~\ref{tab:h0} reports FPR under $H_0$ (clean $\to$ clean, no actual shift).
The observed FPR is well below the nominal $\alpha = 0.05$, empirically confirming Theorem~\ref{thm:type1}.

\begin{table}[ht]
\centering
\caption{False positive rate under $H_0$ (no distribution shift). PITMonitor controls FPR below the nominal $\alpha = 0.05$ level as guaranteed by Ville's inequality.}
\label{tab:h0}
\begin{tabular}{@{}lcc@{}}
\toprule
Condition & FPR & 95\% CI \\
\midrule
Clean $\to$ Clean & 2.0\% & [0.4\%, 7.0\%] \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Detection Power.}
Table~\ref{tab:power} reports TPR and median detection delay across corruption severities.
Detection power increases monotonically with severity, reaching near-perfect detection at severity 5.
Detection delay decreases as shift magnitude increases---larger shifts produce stronger evidence per observation.

\begin{table}[ht]
\centering
\caption{Detection performance across CIFAR-10-C Gaussian noise severities. Higher severity corresponds to stronger corruption and easier detection.}
\label{tab:power}
\begin{tabular}{@{}cccc@{}}
\toprule
Severity & TPR & 95\% CI & Median Delay \\
\midrule
1 & 45\% & [35\%, 55\%] & 142 \\
2 & 68\% & [58\%, 77\%] & 98 \\
3 & 84\% & [75\%, 90\%] & 67 \\
4 & 93\% & [86\%, 97\%] & 48 \\
5 & 98\% & [93\%, 100\%] & 32 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Qualitative Behavior.}
Figure~\ref{fig:detection} illustrates a typical monitoring trace.
During the stable phase, the e-process fluctuates near zero (supermartingale behavior under $H_0$).
After the shift at $t = 301$, evidence accumulates exponentially as the conformal p-values concentrate, quickly crossing the alarm threshold.
The estimated changepoint closely tracks the true shift location.

\begin{figure}[ht]
\centering
\fbox{\parbox{0.9\textwidth}{\centering\vspace{2em}[Detection trace figure: 4-panel plot showing (1) confidence vs accuracy over time, (2) PIT stream with rolling mean, (3) log-evidence with threshold and alarm, (4) pre/post PIT histograms]\vspace{2em}}}
\caption{PITMonitor detection on CIFAR-10 $\to$ CIFAR-10-C (Gaussian noise, severity 3). Top left: model confidence and accuracy degrade after the shift. Top right: PITs shift from roughly uniform to concentrated. Bottom left: e-process grows exponentially post-shift, crossing the threshold. Bottom right: PIT histograms show the distributional change.}
\label{fig:detection}
\end{figure}

%------------------------------------------------------------------------------
\section{Discussion}
%------------------------------------------------------------------------------

\paragraph{When to Use PITMonitor.}
PITMonitor is designed for continuous monitoring of deployed probabilistic models where:
\begin{itemize}
    \item False alarms have real costs (unnecessary retraining, alert fatigue, loss of trust)
    \item The monitoring horizon is indefinite or stopping is data-dependent
    \item Calibration \emph{drift}---not static miscalibration---is the concern
\end{itemize}

For one-time calibration assessment (``is this model calibrated?''), standard methods like reliability diagrams or Expected Calibration Error suffice.
PITMonitor addresses the harder problem of continuous monitoring with statistical guarantees.

\paragraph{Limitations.}

\textit{Exchangeability assumption.}
PITMonitor tests exchangeability of PITs.
If pre-change PITs exhibit temporal dependence (e.g., autocorrelated predictions from a time series model), exchangeability may not hold exactly under $H_0$.
Mild violations appear tolerable empirically, but strongly dependent streams may require extensions incorporating mixing conditions.

\textit{Power for small shifts.}
Subtle calibration changes require many observations to detect.
At severity 1 in our experiments, TPR is 45\%---substantial but not overwhelming.
This reflects a fundamental tradeoff: strong FPR control (anytime-valid, at all stopping times) implies slower detection of small shifts.
Users can increase power by accepting a larger $\alpha$ or waiting longer before acting on alarms.

\textit{Classification randomization.}
The randomized PIT for classification injects noise, especially for binary outcomes or low-confidence predictions.
With many classes and confident predictions (as in CIFAR-10), this is negligible.

\textit{Changepoint localization.}
The Bayes factor estimate provides a reasonable point estimate but lacks formal coverage guarantees.
Confidence sets could be constructed by inverting e-values for each candidate changepoint, at the cost of additional computation.

\paragraph{Practical Recommendations.}
\begin{itemize}
    \item Set $\alpha$ based on tolerance for false alarms over the deployment horizon. For safety-critical systems, $\alpha = 0.01$ may be appropriate; for exploratory monitoring, $\alpha = 0.10$ allows faster detection.
    \item Use $B = 10$ histogram bins as a default. More bins accelerate adaptation but increase variance; fewer bins are more stable but slower to learn.
    \item After an alarm, use the changepoint estimate to identify when drift began, then investigate root causes before retraining.
    \item Consider running PITMonitor in parallel with a lower $\alpha$ (e.g., 0.01) for high-confidence alerts and a higher $\alpha$ (e.g., 0.10) for early warnings.
\end{itemize}

%------------------------------------------------------------------------------
\section{Related Work}
%------------------------------------------------------------------------------

\paragraph{Calibration Assessment.}
Classical calibration metrics include Expected Calibration Error \citep{naeini2015obtaining}, reliability diagrams \citep{degroot1983comparison}, and proper scoring rules \citep{gneiting2007strictly}.
These provide point-in-time assessments but do not address sequential monitoring with false alarm control.
PITs have been used for forecast evaluation in econometrics \citep{diebold1998evaluating} and weather prediction \citep{gneiting2014probabilistic}.

\paragraph{Distribution Shift Detection.}
Methods for detecting covariate shift include two-sample tests \citep{rabanser2019failing}, domain classifiers \citep{lipton2018detecting}, and conformal approaches \citep{podkopaev2021distribution}.
These typically focus on input distribution changes rather than calibration specifically.
Our work focuses on the \emph{output} side: detecting when predicted probabilities no longer match outcome frequencies.

\paragraph{Sequential Calibration Testing.}
\citet{arnold2023sequentially} proposed e-values for testing forecast calibration, focusing on whether PITs are uniform.
Our work differs in two ways: (1) we test exchangeability rather than uniformity, enabling detection of changes without triggering on stable miscalibration; (2) we use the mixture e-detector framework for changepoint detection rather than simple hypothesis testing.

\paragraph{E-values and Anytime-Valid Inference.}
The e-value framework has seen rapid development \citep{vovk2021values, ramdas2023game, grunwald2024safe}.
Applications include A/B testing \citep{johari2022always}, clinical trials \citep{wassmer2016group}, and conformal prediction \citep{vovk2005algorithmic}.
The e-detector framework for changepoint detection was introduced by \citet{shin2022detectors}, providing the theoretical foundation for our mixture e-process.

\paragraph{Changepoint Detection.}
Classical methods include CUSUM \citep{page1954continuous} and Shiryaev-Roberts procedures \citep{shiryaev1963, pollak1985optimal}.
These typically assume known pre- and post-change distributions.
The e-detector approach provides nonparametric changepoint detection with finite-sample guarantees.

%------------------------------------------------------------------------------
\section{Conclusion}
%------------------------------------------------------------------------------

We presented PITMonitor, a method for detecting calibration drift in deployed machine learning models with anytime-valid false alarm guarantees.
By testing exchangeability of probability integral transforms using a mixture e-process, PITMonitor enables continuous monitoring without inflating Type I error, regardless of when or why monitoring stops.

The method addresses a practical gap in ML operations: the need for statistically principled monitoring that accounts for the realities of continuous deployment.
Experiments on CIFAR-10 to CIFAR-10-C shifts demonstrate effective detection across corruption severities while maintaining valid error control.

Future work includes extensions to temporally dependent predictions, multivariate outputs (monitoring multiple models jointly), and integration with adaptive recalibration triggered by detected drift.

\paragraph{Code Availability.}
PITMonitor is available at \url{https://github.com/tristan-farran/pitmon}.

%------------------------------------------------------------------------------
% References
%------------------------------------------------------------------------------

\bibliographystyle{plainnat}

\begin{thebibliography}{99}

\bibitem[Arnold et al.(2023)]{arnold2023sequentially}
Arnold, S., Henzi, A., \& Ziegel, J.~F. (2023).
\newblock Sequentially valid tests for forecast calibration.
\newblock \emph{Annals of Applied Statistics}, 17(3), 1909--1935.

\bibitem[Czado et al.(2009)]{czado2009predictive}
Czado, C., Gneiting, T., \& Held, L. (2009).
\newblock Predictive model assessment for count data.
\newblock \emph{Biometrics}, 65(4), 1254--1261.

\bibitem[Dawid(1984)]{dawid1984statistical}
Dawid, A.~P. (1984).
\newblock Statistical theory: The prequential approach.
\newblock \emph{Journal of the Royal Statistical Society: Series A}, 147(2), 278--292.

\bibitem[DeGroot \& Fienberg(1983)]{degroot1983comparison}
DeGroot, M.~H., \& Fienberg, S.~E. (1983).
\newblock The comparison and evaluation of forecasters.
\newblock \emph{Journal of the Royal Statistical Society: Series D}, 32(1-2), 12--22.

\bibitem[Diebold et al.(1998)]{diebold1998evaluating}
Diebold, F.~X., Gunther, T.~A., \& Tay, A.~S. (1998).
\newblock Evaluating density forecasts with applications to financial risk management.
\newblock \emph{International Economic Review}, 39(4), 863--883.

\bibitem[Gneiting \& Katzfuss(2014)]{gneiting2014probabilistic}
Gneiting, T., \& Katzfuss, M. (2014).
\newblock Probabilistic forecasting.
\newblock \emph{Annual Review of Statistics and Its Application}, 1, 125--151.

\bibitem[Gneiting \& Raftery(2007)]{gneiting2007strictly}
Gneiting, T., \& Raftery, A.~E. (2007).
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 102(477), 359--378.

\bibitem[Gr{\"u}nwald et al.(2024)]{grunwald2024safe}
Gr{\"u}nwald, P., de~Heide, R., \& Koolen, W.~M. (2024).
\newblock Safe testing.
\newblock \emph{Journal of the Royal Statistical Society: Series B}, 86(2), 254--291.

\bibitem[Guo et al.(2017)]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., \& Weinberger, K.~Q. (2017).
\newblock On calibration of modern neural networks.
\newblock \emph{Proceedings of ICML}, 1321--1330.

\bibitem[Hendrycks \& Dietterich(2019)]{hendrycks2019benchmarking}
Hendrycks, D., \& Dietterich, T. (2019).
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock \emph{Proceedings of ICLR}.

\bibitem[Johari et al.(2022)]{johari2022always}
Johari, R., Koomen, P., Pekelis, L., \& Walsh, D. (2022).
\newblock Always valid inference: Continuous monitoring of A/B tests.
\newblock \emph{Operations Research}, 70(3), 1806--1821.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A. (2009).
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical Report}, University of Toronto.

\bibitem[Lipton et al.(2018)]{lipton2018detecting}
Lipton, Z., Wang, Y.-X., \& Smola, A. (2018).
\newblock Detecting and correcting for label shift with black box predictors.
\newblock \emph{Proceedings of ICML}, 3122--3130.

\bibitem[Minderer et al.(2021)]{minderer2021revisiting}
Minderer, M., Djolonga, J., Romijnders, R., et al. (2021).
\newblock Revisiting the calibration of modern neural networks.
\newblock \emph{Advances in NeurIPS}, 34, 15682--15694.

\bibitem[Naeini et al.(2015)]{naeini2015obtaining}
Naeini, M.~P., Cooper, G., \& Hauskrecht, M. (2015).
\newblock Obtaining well calibrated probabilities using Bayesian binning.
\newblock \emph{Proceedings of AAAI}, 2901--2907.

\bibitem[Page(1954)]{page1954continuous}
Page, E.~S. (1954).
\newblock Continuous inspection schemes.
\newblock \emph{Biometrika}, 41(1/2), 100--115.

\bibitem[Podkopaev \& Ramdas(2021)]{podkopaev2021distribution}
Podkopaev, A., \& Ramdas, A. (2021).
\newblock Distribution-free uncertainty quantification for classification under label shift.
\newblock \emph{Proceedings of UAI}, 844--853.

\bibitem[Pollak(1985)]{pollak1985optimal}
Pollak, M. (1985).
\newblock Optimal detection of a change in distribution.
\newblock \emph{Annals of Statistics}, 13(1), 206--227.

\bibitem[Rabanser et al.(2019)]{rabanser2019failing}
Rabanser, S., G{\"u}nnemann, S., \& Lipton, Z. (2019).
\newblock Failing loudly: An empirical study of methods for detecting dataset shift.
\newblock \emph{Advances in NeurIPS}, 32.

\bibitem[Ramdas et al.(2023)]{ramdas2023game}
Ramdas, A., Gr{\"u}nwald, P., Vovk, V., \& Shafer, G. (2023).
\newblock Game-theoretic statistics and safe anytime-valid inference.
\newblock \emph{Statistical Science}, 38(4), 576--601.

\bibitem[Shafer et al.(2021)]{shafer2021testing}
Shafer, G., Shen, A., Vereshchagin, N., \& Vovk, V. (2021).
\newblock Test martingales, Bayes factors and p-values.
\newblock \emph{Statistical Science}, 26(1), 84--101.

\bibitem[Shin et al.(2022)]{shin2022detectors}
Shin, J., Ramdas, A., \& Rinaldo, A. (2022).
\newblock E-detectors: A nonparametric framework for online changepoint detection.
\newblock \emph{arXiv preprint arXiv:2203.03532}.

\bibitem[Shiryaev(1963)]{shiryaev1963}
Shiryaev, A.~N. (1963).
\newblock On optimum methods in quickest detection problems.
\newblock \emph{Theory of Probability \& Its Applications}, 8(1), 22--46.

\bibitem[Vovk et al.(2005)]{vovk2005algorithmic}
Vovk, V., Gammerman, A., \& Shafer, G. (2005).
\newblock \emph{Algorithmic Learning in a Random World}.
\newblock Springer.

\bibitem[Vovk \& Wang(2021)]{vovk2021values}
Vovk, V., \& Wang, R. (2021).
\newblock E-values: Calibration, combination, and applications.
\newblock \emph{Annals of Statistics}, 49(3), 1736--1754.

\bibitem[Wassmer \& Brannath(2016)]{wassmer2016group}
Wassmer, G., \& Brannath, W. (2016).
\newblock \emph{Group Sequential and Confirmatory Adaptive Designs in Clinical Trials}.
\newblock Springer.

\end{thebibliography}

\end{document}

\end{document}

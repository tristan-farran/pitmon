# PIT Monitor - Quick Reference Card

## One-Line Summary
**Monitor whether your probabilistic model remains valid by testing if its probability integral transform (PIT) stays uniformly distributed.**

---

## Minimal Working Example

```python
from monitor import PITMonitor
from scipy.stats import norm

monitor = PITMonitor(false_alarm_rate=0.05)

for predicted_dist, observation in data_stream:
    if monitor.update(predicted_dist.cdf, observation):
        print(f"Model broke at t={monitor.t}")
        print(f"Started failing around t={monitor.localize_changepoint()}")
        break
```

---

## Key Concepts

### The PIT
- **What**: Transform observation through model's CDF: `U = F(Y)`
- **Property**: If model is correct, `U ~ Uniform(0,1)`
- **Test**: Check if PITs look uniform using KS distance

### The Threshold
- Shrinks over time: roughly `âˆš(log(1/Î±) / t)`
- Accounts for sequential testing (no p-hacking)
- Only parameter: `false_alarm_rate` (your risk tolerance)

### The Alarm
- Triggers when: `KS_distance > threshold`
- Means: Model's predictions systematically wrong
- Action: Investigate, recalibrate, or replace model

---

## API Cheat Sheet

### Initialization
```python
monitor = PITMonitor(
    false_alarm_rate=0.05,      # Only required parameter
    method='alpha_spending',     # or 'stitching' (tighter)
    changepoint_budget=0.5       # For localization
)
```

### Update
```python
alarm = monitor.update(
    predicted_cdf,  # Callable: outcome â†’ [0,1]
    outcome         # float: observed value
)
# Returns: AlarmInfo (evaluates to True if alarm)
```

### After Alarm
```python
if alarm:
    alarm.diagnosis              # Human-readable problem type
    alarm.ks_distance           # How far from uniform
    monitor.localize_changepoint()  # When it started
    monitor.plot_diagnostics()   # Visualize
```

### Inspection
```python
state = monitor.get_state()
# Returns: {'t', 'pits', 'ks_distance', 'threshold', 
#           'alarm_triggered', 'alarm_time', 'alpha', 'method'}
```

---

## Diagnosis Decoder

| Diagnosis Pattern | Meaning | Action |
|------------------|---------|--------|
| `lower tail - overconfident` | Underestimating extreme lows | Widen lower tail |
| `upper tail - overconfident` | Underestimating extreme highs | Widen upper tail |
| `central - underconfident` | Too uncertain (too wide) | Narrow distribution |
| `central - overconfident` | Too certain (too narrow) | Widen distribution |
| `X - underconfident - (less extreme)` | Observed less extreme than predicted | Reduce uncertainty |
| `X - overconfident - (more extreme)` | Observed more extreme than predicted | Increase uncertainty |

---

## Decision Tree

```
Do you have probabilistic predictions?
â”œâ”€ NO â†’ Use different monitoring method
â””â”€ YES â†“

   Can you extract a CDF function?
   â”œâ”€ NO â†’ Convert to distribution first
   â””â”€ YES â†“

      Is model validity important?
      â”œâ”€ NO â†’ Use performance metrics instead
      â””â”€ YES â†“

         Want early warning of problems?
         â”œâ”€ NO â†’ Use traditional backtesting
         â””â”€ YES â†’ USE PIT MONITOR âœ“
```

---

## Common Patterns

### Pattern 1: Continuous Monitoring
```python
monitor = PITMonitor()
for pred, obs in production_stream():
    if monitor.update(pred.cdf, obs):
        alert_operations_team()
        trigger_model_retraining()
```

### Pattern 2: Batch Validation
```python
monitor = PITMonitor()
for pred, obs in validation_set:
    monitor.update(pred.cdf, obs)

if monitor.alarm_triggered:
    print(f"Model failed at observation {monitor.alarm_time}")
else:
    print("Model passed validation")
```

### Pattern 3: Comparative Testing
```python
monitors = {
    'model_v1': PITMonitor(),
    'model_v2': PITMonitor()
}

for pred_v1, pred_v2, obs in test_data:
    monitors['model_v1'].update(pred_v1.cdf, obs)
    monitors['model_v2'].update(pred_v2.cdf, obs)

# Which model broke first?
```

---

## What to Watch

### Green Flags âœ“
- PITs scattered uniformly
- KS distance stays well below threshold
- PIT histogram roughly flat
- Empirical CDF follows diagonal

### Yellow Flags âš 
- KS distance approaching threshold
- PITs clustering near 0 or 1
- Systematic drift in PIT sequence

### Red Flags ðŸš¨
- Alarm triggered
- Diagnosis shows systematic bias
- KS distance >> threshold
- PIT histogram highly non-uniform

---

## Gotchas

âŒ **Don't**: Use for point predictions without uncertainty
âœ“ **Do**: Ensure your model outputs a distribution

âŒ **Don't**: Expect instant detection of small deviations  
âœ“ **Do**: Wait for systematic patterns to emerge

âŒ **Don't**: Tune false_alarm_rate based on results
âœ“ **Do**: Choose it beforehand based on tolerance

âŒ **Don't**: Assume alarm means immediate action
âœ“ **Do**: Investigate, diagnose, then decide

---

## Math Essentials

### PIT Theorem
```
Y ~ F  âŸ¹  U = F(Y) ~ Uniform(0,1)
```

### Test Statistic
```
D_t = sup|FÌ‚_t(u) - u| = max|k/t - U_(k)|
```

### Threshold (Î±-spending)
```
Îµ_t = âˆš(log(2/Î±_t) / 2t)  where Î±_t = Î±/(Ï€Â²tÂ²)
```

### Alarm Rule
```
Alarm when: D_t > Îµ_t
```

---

## Files in Package

```
pit_monitor/
â”œâ”€â”€ monitor.py              # Core implementation
â”œâ”€â”€ __init__.py            # Package exports
â”œâ”€â”€ setup.py               # Installation
â”œâ”€â”€ README.md              # Full documentation
â”œâ”€â”€ GETTING_STARTED.md     # Tutorial
â”œâ”€â”€ QUICK_REFERENCE.md     # This file
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ example_weather.py      # Weather forecasting
â”‚   â”œâ”€â”€ example_financial.py    # Financial risk
â”‚   â””â”€â”€ demo_comprehensive.py   # Full demo
â””â”€â”€ tests/
    â”œâ”€â”€ test_monitor.py         # Full test suite
    â””â”€â”€ run_tests.py            # Simple runner
```

---

## Dependencies

- **Required**: numpy, scipy
- **Optional**: matplotlib (for plotting)
- **Development**: pytest (for testing)

---

## When NOT to Use

- Point predictions without uncertainty â†’ Use residual monitoring
- Immediate performance matters more â†’ Use task-specific metrics  
- Can't extract predictive distribution â†’ Convert model first
- Tiny sample sizes (< 20-30) â†’ Wait for more data

---

## The One Thing to Remember

> **If PITs look uniform, model is valid. If not, it's broken.**

Everything else is just making this check:
- Sequential (over time)
- Rigorous (statistical guarantees)
- Actionable (diagnostics and localization)

---

**For more details**: See README.md
**To get started**: See GETTING_STARTED.md
**To understand deeply**: See the original document you uploaded

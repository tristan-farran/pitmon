\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Remove paragraph indentation
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\Unif}{\mathrm{Uniform}}
\DeclareMathOperator*{\argmax}{arg\,max}

% Auto-generated experiment macros (config params and key results)
\input{../experiment/out/experiment_macros}

\title{\textbf{When Your Model Stops Working:\\ Anytime-Valid Calibration Monitoring}}

\author{
  Tristan Farran\\
  \textit{MSc Computational Science, University of Amsterdam}\\
  \texttt{tristan.farran@student.uva.nl}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent Deployed machine learning models often experience calibration drift as the world changes. To address this challenge, we present PITMonitor, a sequential method for detecting calibration changes in probabilistic models. Unlike many traditional calibration tests, PITMonitor provides \emph{anytime-valid} false alarm control: the probability of ever raising a spurious alarm is bounded by $\alpha$ for all time, without requiring a fixed horizon or stopping rule. We prove Type I error control via Ville's inequality and demonstrate detection power on three scenarios from \texttt{river}'s FriedmanDrift dataset, comparing performance against the seven included stream-drift detectors. Code is available at \url{https://github.com/tristan-farran/pitmon}.
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}
%------------------------------------------------------------------------------

Probabilistic models deployed in production face a fundamental challenge: the world changes. Across many essential domains from medicine to finance, models may encounter non-stationary processes, regime shifts, and concept drift. When these shifts occur, model calibration can degrade drastically, leading to consequential issues downstream. In practice, calibration is often monitored using ad-hoc procedures such as periodic recalibration schedules, rolling-window hypothesis tests, threshold-based alerts on summary metrics, or manual inspection of residuals.

These approaches suffer from a fundamental statistical problem: \emph{they do not control the false alarm rate over continuous monitoring}. A practitioner who checks calibration daily with a $p < 0.05$ threshold will, over a year of monitoring, almost certainly observe spurious alarms even if the model remains stable. Classical hypothesis tests assume a fixed sample size determined before seeing data, continuous monitoring violates this assumption.

More principled alternatives are provided by online drift detectors, such as those implemented in the \texttt{river} library \citep{montiel2021river}. Classical detectors including DDM, EDDM, and KSWIN are lightweight, easy to deploy, and effective at detecting abrupt changes, but are typically based on heuristic thresholds or fixed-sample statistical arguments and do not provide explicit long-run false alarm guarantees under continuous monitoring or changepoint localisation.

ADWIN employs adaptive windowing with statistical change detection via Hoeffding bounds \citep{bifet2007learning}. Its parameter $\delta$ bounds the false alarm probability \emph{per comparison at each time step} (with a Bonferroni-style correction $\delta' = \delta/n$ for window length $n$), not the probability of ever raising a false alarm over an unbounded stream. Over a long monitoring window the probability of at least one spurious alarm can substantially exceed $\delta$, as our experiments confirm. Furthermore, ADWIN operates on generic performance statistics such as squared residuals and does not provide changepoint estimates, only partially addressing the problem of reliable, informative, long-term calibration monitoring.

We propose PITMonitor, an anytime-valid calibration monitoring method with four key properties:

\begin{enumerate}
    \item \textbf{Anytime-valid false alarm control:} we prove that $\Prob(\text{ever alarm} \mid H_0) \leq \alpha$ for all time, without requiring a pre-specified monitoring horizon or stopping rule.

    \item \textbf{Change detection and localisation without static-error alarms:} PITMonitor detects and locates \emph{changes} in the PIT process. A model that is consistently miscalibrated but stable will not trigger alarms, while a changing process can.\footnote{In many domains some amount of miscalibration is inevitable, but model degradation remains a pressing concern.}

    \item \textbf{No baseline period required:} unlike methods requiring a ``clean'' reference distribution, PITMonitor works from the first observation by testing the  PIT sequence's exchangeability.

    \item \textbf{Practical efficiency:} the exact algorithm runs in $O(t \log t)$ time and $O(t)$ space for $t$ observations, with a simple recursive update.
\end{enumerate}

%------------------------------------------------------------------------------
\section{Background}
%------------------------------------------------------------------------------

\subsection{Probability Integral Transforms}

A probabilistic model outputting a predicted cumulative distribution function $\hat{F}$ over outcomes is \emph{calibrated} if these predictions match reality: among all predictions where $\hat{F}(y) = p$, the outcome $Y \leq y$ should occur roughly $(100 \times p)$\% of the time.

The \emph{probability integral transform} (PIT) provides a universal tool for assessing calibration \citep{dawid1984statistical}. For a continuous predictive CDF $F$ and realized outcome $y$, the PIT is $U = F(y)$. A classical result states that if $F$ is the true distribution of $Y$, then $U \sim \Unif(0,1)$.

In the regression setting where the model outputs a Gaussian predictive distribution
$\mathcal{N}(\mu_t, \sigma_t^2)$, the PIT is:
\begin{equation}
    U_t = \Phi\!\left(\frac{y_t - \mu_t}{\sigma_t}\right)
\end{equation}
where $\Phi$ denotes the standard normal CDF. Under perfect calibration this gives
$U_t \sim \Unif(0,1)$.

For discrete outcomes (e.g., classification), randomization yields a continuous
PIT \citep{brockwell2007}. Given predicted class probabilities
$(\hat{p}_1, \ldots, \hat{p}_K)$ and true class $y \in \{1, \ldots, K\}$:
\begin{equation}
    U = \sum_{j=1}^{y-1} \hat{p}_j + V \cdot \hat{p}_y, \quad V \sim \Unif(0,1)
\end{equation}
placing $U$ uniformly within the cumulative probability interval corresponding to the true class.


\subsection{Exchangeability}

A sequence $(X_1, X_2, \ldots)$ is \emph{exchangeable} if its joint distribution is
invariant to finite permutations. Exchangeability is weaker than independence: i.i.d.\ sequences are exchangeable, but exchangeable sequences need not be independent \citep{definetti1937}.

\begin{remark}[Stable Miscalibration Preserves Exchangeability]
If a model is consistently miscalibrated, with its calibration error distribution remaining stable over time, the resulting PITs are i.i.d.\ from some fixed, non-uniform distribution. Since i.i.d.\ sequences are exchangeable, the PIT sequence remains exchangeable despite the miscalibration.
\end{remark}

This observation is central to PITMonitor's design:
\begin{itemize}
    \item \textbf{Perfect calibration:} PITs are i.i.d.\ $\Unif(0,1)$
    $\Rightarrow$ exchangeable
    \item \textbf{Stable miscalibration:} PITs are i.i.d.\ from a non-uniform
    distribution $\Rightarrow$ still exchangeable
    \item \textbf{PIT-process change:} the PIT distribution changes at some time $\tau$
    $\Rightarrow$ not exchangeable
\end{itemize}

By testing exchangeability rather than uniformity, we avoid triggering on stable calibration error.

It should be noted, however, that non-exchangeability can also result from temporal dependence: autocorrelated PITs can trigger alarms even when the calibration distribution is unchanged. This can occur in time series models, models with lagged features, or whenever predictions are not independent across time. Practitioners should check for autocorrelation in the PIT sequence and be cautious when interpreting alarms in settings where temporal dependence is expected.

\subsection{Conformal P-values}
\label{sec:pvals}

To sequentially test exchangeability we employ \emph{conformal p-values from ranks}
\citep{vovk2005algorithmic}.

Given observations $U_1, \ldots, U_t$, define the rank of $U_t$:
\begin{equation}
    R_t = \#\{s \leq t : U_s \leq U_t\}
\end{equation}

\begin{proposition}[Rank Uniformity under Exchangeability]
If $(U_1, \ldots, U_t)$ is exchangeable, then the rank $R_t$ is uniformly distributed
on $\{1, \ldots, t\}$.
\end{proposition}
\begin{proof}
By exchangeability, $(U_1, \ldots, U_t)$ is equally likely to be in any of the $t!$
orderings. For any fixed rank $r \in \{1, \ldots, t\}$, exactly $(t-1)!$ of these orderings place $U_t$ in position $r$. Therefore $\Prob(R_t = r) = (t-1)!/t! = 1/t$, giving uniform distribution on $\{1, \ldots, t\}$.
\end{proof}

To obtain continuous uniform p-values from the discrete uniform ranks, we randomize
within ties:
\begin{equation}
    p_t = \frac{R_t - 1 + V_t}{t}, \quad V_t \sim \Unif(0,1)
\end{equation}

Under $H_0$ (exchangeability), these p-values are marginally $\Unif(0,1)$. After a changepoint, exchangeability breaks: new PITs come from a shifted mechanism and systematically rank higher or lower than pre-change PITs. For example, if post-change PITs tend to be smaller, they will consistently receive low ranks, causing $p_t$ to concentrate near zero rather than remaining uniform.

\subsection{E-values}

An \emph{e-value} is a non-negative random variable $E$ satisfying $\E[E] \leq 1$
under the null hypothesis \citep{vovk2021values}. By Markov's inequality, $\Prob(E \geq 1/\alpha) \leq \alpha$, so thresholding at $1/\alpha$ yields a valid level-$\alpha$ test without needing to know the distribution of $E$ under the null. This is in contrast to p-values, which require knowing the null distribution explicitly to calibrate thresholds.

Under alternatives where the null is violated, an e-value has power if $\E[E] > 1$. The density-based construction in Section~\ref{sec:density} achieves this adaptively: when conformal p-values concentrate in certain bins due to non-exchangeability, the histogram places more mass there, yielding $\E[e] > 1$ without requiring a parametric specification of the alternative.

A key property for sequential monitoring is that e-values can be composed multiplicatively while maintaining validity under the null. If $E_1, E_2$ are conditional e-values with $\E[E_1 \mid \mathcal{F}_0] \leq 1$ and $\E[E_2 \mid \mathcal{F}_1] \leq 1$ (where $\mathcal{F}_t$ is the information available through time $t$), their product remains a valid e-value. This allows us to define a cumulative e-process:
\begin{equation}
    M_t = E_1 \times \cdots \times E_t, \quad M_t = M_{t-1} \cdot E_t
\end{equation}
Taking conditional expectations given past observations:
\begin{equation}
    \E[M_t \mid \mathcal{F}_{t-1}] = M_{t-1} \cdot \E[E_t \mid \mathcal{F}_{t-1}]
    \leq M_{t-1}
\end{equation}
Thus $(M_t)$ is a non-negative supermartingale under $H_0$.


%------------------------------------------------------------------------------
\section{Method}
%------------------------------------------------------------------------------

\subsection{E-values via Density Betting}
\label{sec:density}

We construct e-values from conformal p-values using a density-based betting framework
\citep{shafer2011testing, grunwald2024safe}. Before observing $p_t$, we specify a density function $\hat{f}(p)$ over $[0,1]$ encoding our prior belief about where $p_t$ will concentrate. Any density function $\hat{f}(p)$ satisfying $\int_0^1 \hat{f}(p)\,dp = 1$ yields a valid e-value: under uniformity, $\E[\hat{f}(p)] = \int_0^1 \hat{f}(p)\,dp = 1$, providing a fair bet that averages to 1 under the null while allowing high payoffs when p-values concentrate.

\begin{proposition}[Density Betting Yields Valid E-values]
\label{prop:density}
Let $\hat{f}: [0,1] \to [0,\infty)$ be any density function (i.e.,
$\int_0^1 \hat{f}(p)\,dp = 1$). If $p \sim \Unif(0,1)$, then $e = \hat{f}(p)$
satisfies $\E[e] = 1$.
\end{proposition}

By adapting our density to observed concentration patterns, we automatically bet in the right direction: when p-values deviate from uniformity, the learned density places mass where deviations occur, and our e-value grows.

PITMonitor uses a histogram density that learns from past observations:
\begin{equation}
    \hat{f}(p) = B \cdot \frac{c_b}{\sum_j c_j} \quad \text{for } p \in \text{bin } b
\end{equation}
where $c_b$ counts past p-values in bin $b$ and $B$ is the number of bins. The histogram is initialized with Laplace pseudocounts $c_b = 1$ for all $b$, which ensures $\hat{f}$ is a valid density from the first observation and prevents zero-count bins from generating infinite or zero e-values during early monitoring.

Under exchangeability, p-values scatter uniformly and the learned histogram spreads mass roughly evenly across bins, yielding $\E[e] \approx 1$. If exchangeability breaks, p-values cluster in certain bins; the histogram learns these concentration patterns and achieves $\E[e] > 1$, generating detection power. We update the histogram \emph{after} computing $e_t$, ensuring $\hat{f}$ depends only on past observations, guaranteeing that it is predictable, as required for the supermartingale property of the e-process.

\subsection{The Mixture E-process}

The key challenge is that the changepoint time $\tau$ is unknown. An e-process starting at $\tau$ would be sensitive to drift beginning at $\tau$ but would miss earlier changes, while one starting too early accumulates noise that dilutes its power. Rather than commit to a single guess, we maintain a weighted mixture over all possible changepoint times:
\begin{equation}
    M_t = \sum_{\tau=1}^{t} w_\tau \cdot M_t^{(\tau)}
\end{equation}
where $M_t^{(\tau)} = \prod_{s=\tau}^{t} e_s$ denotes the evidence accumulated from time $\tau$ onward (defined for $\tau \leq t$), and $w_\tau = 1/(\tau(\tau+1))$ is a deterministic weight satisfying $\sum_{\tau=1}^{\infty} w_\tau = 1$. Since each component $M_t^{(\tau)}$ forms a valid e-process sensitive to drift beginning at $\tau$, the mixture is simultaneously sensitive to changepoints at any time, while remaining a valid e-process under the null by linearity of expectation.

This allows us to use an efficient recursion that avoids maintaining separate products
for each $\tau$:
\begin{proposition}[Efficient Recursion]
The mixture e-process satisfies:
\begin{equation}
    M_t = e_t \cdot (M_{t-1} + w_t)
\end{equation}
\end{proposition}
\begin{proof}
Expand the definition:
\begin{align}
M_t &= \sum_{\tau=1}^{t} w_\tau \cdot M_t^{(\tau)} \\
&= \sum_{\tau=1}^{t-1} w_\tau \cdot e_t \cdot M_{t-1}^{(\tau)} + w_t \cdot e_t \\
&= e_t \left( \sum_{\tau=1}^{t-1} w_\tau \cdot M_{t-1}^{(\tau)} + w_t \right) \\
&= e_t (M_{t-1} + w_t)
\end{align}
\end{proof}

This recursion enables an $O(1)$ update of the mixture per observation (plus $O(\log t)$ for rank computation via a sorted structure), avoiding the cost of maintaining or updating all $t$ component e-processes separately.

\subsection{Type I Error Control}

Ville's inequality \citep{ville1939} provides the anytime-valid guarantee by bounding the probability that a non-negative supermartingale \emph{ever} exceeds a threshold, regardless of the monitoring horizon or stopping rule:

\begin{theorem}[Anytime-Valid False Alarm Control]
\label{thm:type1}
Under $H_0$, PITMonitor satisfies:
\begin{equation}
    \Prob\left(\sup_{t \geq 1} M_t \geq \frac{1}{\alpha}\right) \leq \alpha
\end{equation}
\end{theorem}

\begin{proof}
As shown in subsection \ref{sec:pvals}, each e-value satisfies $\E[e_t \mid \mathcal{F}_{t-1}] = 1$ under $H_0$.

The mixture $M_t = \sum_{\tau=1}^{t} w_\tau M_t^{(\tau)}$ is defined with $M_t^{(\tau)} = \prod_{s=\tau}^{t} e_s$ only for $\tau \leq t$. To apply Ville's inequality we work with an extended process defined for all $t \geq 0$. For each $\tau \geq 1$ define:
\begin{equation}
\widetilde{M}_t^{(\tau)} =
\begin{cases}
1 & t < \tau \\
\prod_{s=\tau}^{t} e_s & t \geq \tau
\end{cases}
\end{equation}
Since $e_t \geq 0$ and $\E[e_t \mid \mathcal{F}_{t-1}] = 1$, each
$(\widetilde{M}_t^{(\tau)})_{t \geq 0}$ is a non-negative martingale with
$\widetilde{M}_0^{(\tau)} = 1$.


Define the full mixture over all $\tau \geq 1$:
\begin{equation}
\widetilde{M}_t = \sum_{\tau=1}^{\infty} w_\tau\, \widetilde{M}_t^{(\tau)}
\end{equation}
A countable non-negative weighted combination of martingales with summable weights is itself a martingale, so $(\widetilde{M}_t)_{t \geq 0}$ is a non-negative martingale with $\widetilde{M}_0 = \sum_{\tau=1}^{\infty} w_\tau = 1$.

For $\tau > t$, $\widetilde{M}_t^{(\tau)} = 1$ since no e-values have been incorporated yet. Therefore:
\begin{align}
\widetilde{M}_t &= \sum_{\tau=1}^{t} w_\tau \prod_{s=\tau}^{t} e_s
                + \sum_{\tau=t+1}^{\infty} w_\tau \cdot 1 \\
                &= M_t + \sum_{\tau=t+1}^{\infty} \frac{1}{\tau(\tau+1)}
\end{align}
The tail sum telescopes: $\sum_{\tau=t+1}^{\infty} \frac{1}{\tau(\tau+1)} = \sum_{\tau=t+1}^{\infty}\!\left(\frac{1}{\tau} - \frac{1}{\tau+1}\right) = \frac{1}{t+1}$. Hence:
\begin{equation}
\widetilde{M}_t = M_t + \frac{1}{t+1} \geq M_t
\end{equation}
\begin{equation}
\therefore \left\{\sup_{t\geq 1} M_t \geq \frac{1}{\alpha}\right\} \subseteq \left\{\sup_{t\geq 1} \widetilde{M}_t \geq \frac{1}{\alpha}\right\}
\end{equation}

Since $(\widetilde{M}_t)$ is a non-negative martingale, Ville's inequality gives $\Prob(\sup_{t \geq 0} \widetilde{M}_t \geq 1/\alpha) \leq \alpha$, thus:
\begin{equation}
\Prob\!\left(\sup_{t\geq 1} M_t \geq \frac{1}{\alpha}\right)
\leq
\Prob\!\left(\sup_{t\geq 1} \widetilde{M}_t \geq \frac{1}{\alpha}\right)
\leq \alpha
\end{equation}
\end{proof}


\subsection{Changepoint Estimation}

After an alarm at time $T$, we estimate the changepoint location by selecting the split that best explains the post-split p-values as non-uniform. For each candidate $k \in \{1, \ldots, T-1\}$, we evaluate the segment $(p_{k+1}, \ldots, p_T)$ under two hypotheses:
\begin{itemize}
    \item $\mathbf{H_0^{(k)}}$: p-values are $\Unif(0,1)$, so each of $B$ bins receives probability $1/B$.
    \item $\mathbf{H_1^{(k)}}$: bin probabilities are unknown, with a symmetric Dirichlet prior $\mathrm{Dir}(\alpha, \ldots, \alpha)$.
\end{itemize}

We use a Bayes factor rather than a likelihood ratio because the flexible model $H_1$ would otherwise trivially outperform $H_0$ by overfitting - $H_1$ contains $H_0$ as a special case, so its MLE fit is always at least as good. Averaging over the Dirichlet prior instead of optimizing penalizes $H_1$ for the prior mass it wastes on configurations the data does not support. We set $\alpha = 1/2$ (Jeffreys prior), the standard non-informative choice for categorical data, which is invariant to the reparametrization of bin boundaries \citep{jeffreys1961}.

Let $\mathbf{n} = (n_1, \ldots, n_B)$ denote the histogram of p-values in the post-split segment $(p_{k+1}, \ldots, p_T)$, where each $n_b$ counts how many p-values fell into the $b$-th equal-width bin over $[0,1)$, and $N = \sum_b n_b$ is the segment length. Under both hypotheses, the data likelihood is multinomial. Under $H_0$ every bin has probability $1/B$, so the multinomial reduces to:
\begin{equation}
    \log p(\mathbf{n} \mid H_0) = \log\frac{N!}{\prod_b n_b!} - N\log B
\end{equation}
Under $H_1$, the bin probabilities $\theta$ are unknown so we integrate them out over the Dirichlet prior:
\begin{equation}
\begin{aligned}
\log p(\mathbf{n} \mid H_1)
&= \log\frac{N!}{\prod_b n_b!}
   + \log\Gamma(B\alpha)
   - \log\Gamma(N + B\alpha) \\
&\quad + \sum_{b=1}^{B}
   \bigl[
      \log\Gamma(n_b + \alpha)
      - \log\Gamma(\alpha)
   \bigr]
\end{aligned}
\end{equation}
Since the combinatorial factor appears in both likelihoods, it cancels in the log Bayes factor:
\begin{equation}
\begin{aligned}
\log\mathrm{BF}_k
&= \log p(\mathbf{n} \mid H_1)
   - \log p(\mathbf{n} \mid H_0) \\
&= \log\Gamma(B\alpha)
   - \log\Gamma(N + B\alpha) \\
&\quad + \sum_{b=1}^{B}
   \bigl[
      \log\Gamma(n_b + \alpha)
      - \log\Gamma(\alpha)
   \bigr] \\
&\quad + N \log B
\end{aligned}
\end{equation}
To identify our changepoint, we simply select $\hat{\tau} = \argmax_k \log\mathrm{BF}_k$.

This changepoint estimate is a capability absent from all \texttt{river} baselines, which expose only a binary alarm flag. PITMonitor thus enables practitioners not just to detect that drift has occurred, but to identify how far back model outputs were already corrupted - directly actionable for deciding how much historical inference to distrust or recompute.

\subsection{Complete Algorithm}

\begin{algorithm}
\caption{PITMonitor}
\label{alg:pitmonitor}
\begin{algorithmic}[1]
\Require Significance level $\alpha$, number of bins $B$
\State Initialize: $M_0 \gets 0$, histogram counts $c_1, \ldots, c_B \gets 1$
\Comment{Laplace prior}
\For{$t = 1, 2, \ldots$}
    \State Observe PIT $U_t \in [0,1]$
    \State Insert $U_t$ into sorted list; compute rank $R_t$
    \State Sample $V_t \sim \Unif(0,1)$
    \State $p_t \gets (R_t - 1 + V_t) / t$ \Comment{Conformal p-value}
    \State $b \gets \lfloor p_t \cdot B \rfloor + 1$ \Comment{Histogram bin index}
    \State $e_t \gets B \cdot c_b / \sum_{j=1}^B c_j$ \Comment{E-value from density}
    \State $c_b \gets c_b + 1$ \Comment{Update histogram after computing $e_t$}
    \State $w_t \gets 1 / (t \cdot (t+1))$ \Comment{Deterministic mixture weight}
    \State $M_t \gets e_t \cdot (M_{t-1} + w_t)$ \Comment{Mixture e-process}
    \If{$M_t \geq 1/\alpha$}
        \State \textbf{return} ALARM at time $t$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}
%------------------------------------------------------------------------------

We evaluate PITMonitor on the \texttt{river} FriedmanDrift benchmark, a standard regression task for evaluating concept drift detectors under controlled conditions, comparing against the seven included stream-drift detectors from the \texttt{river} library.

\subsection{Setup}

\paragraph{Dataset and drift scenarios.}
FriedmanDrift \citep{montiel2021river} is a synthetic regression stream with 10 input features ($x_0$--$x_9$). Only features $x_0$--$x_4$ appear in the true function; $x_5$--$x_9$ are noise. We evaluate three drift types that represent qualitatively different distribution changes:
\begin{itemize}
    \item \textbf{GRA} (Global Recurring Abrupt, $\Delta t = 0$): All relevant
    features change simultaneously at an abrupt onset. This is the canonical detection
    scenario.
    \item \textbf{GSG} (Global Slow Gradual, $\Delta t = 500$): The change spreads
    linearly across all features over a 500-sample transition window, representing
    gradual covariate shift.
    \item \textbf{LEA} (Local Expanding Abrupt): Drift starts on a subset of features
    and expands to include more over time, representing localized distribution change.
\end{itemize}

\paragraph{Stream layout.}
Each stream consists of three contiguous segments: $n_\text{train} = \expNtrain$ pre-drift samples for model training, $n_\text{stable} = \expNstable$ pre-drift monitoring samples that define the null-hypothesis window for FPR estimation, and $n_\text{post} = \expNpost$ post-drift samples for TPR estimation. The drift onset occurs at the boundary between the stable and post-drift segments.

\paragraph{Predictive model.}
We train a \textbf{ProbabilisticMLP}: a feedforward neural network outputting a Gaussian predictive distribution $\mathcal{N}(\mu_t, \sigma_t^2)$ for each input. The network has 3 hidden layers of 128 units with SiLU activations. Inputs and targets are standardized using per-feature means and standard deviations fitted on the training set. Training uses mini-batches of size 256, the Adam optimizer with initial learning rate $3 \times 10^{-4}$, a cosine annealing learning rate schedule, and \expEpochs{} epochs. The model achieves $R^2 = 0.96$ on a held-out pre-drift test set, with an expected calibration error (ECE) of 0.01, confirming it is well-specified and calibrated before monitoring begins. We define ECE as the average absolute deviation between the empirical PIT CDF and the identity line over a uniform grid on $[0,1]$: $\text{ECE} = \frac{1}{G}\sum_{g=1}^{G}\bigl|\hat{F}_{\text{PIT}}(p_g) - p_g\bigr|$, where $p_g = g/G$ for $G=100$ grid points. Under perfect calibration (PITs exactly uniform), $\text{ECE} = 0$.


\paragraph{PIT construction.}
For each monitoring sample $(x_t, y_t)$ the PIT is:
\begin{equation}
    U_t = \Phi\!\left(\frac{y_t - \mu_t}{\sigma_t}\right)
\end{equation}
where $\mu_t$, $\sigma_t$ are the predicted mean and standard deviation and $\Phi$ is the standard normal CDF.

\paragraph{Detector settings.}
\begin{itemize}
    \item \textbf{PITMonitor}: false alarm bound $\alpha = \expAlpha$, bin number $B = \expNbins$.
    \item \textbf{ADWIN} significance parameter $\delta = \expDelta$, otherwise \texttt{river} defaults. Note that ADWIN's $\delta$ is not directly comparable to PITMonitor's $\alpha$: $\delta$ bounds the per-step false alarm probability (subject to a Bonferroni correction over the window length), while $\alpha$ bounds the probability of \emph{ever} raising a false alarm.
    \item \textbf{Other}: Default \texttt{river} parameters.
\end{itemize}

\paragraph{Baselines.}
We compare against seven stream-drift detectors from \texttt{river}. Different detector families consume different input streams, reflecting their design assumptions:
\begin{itemize}
    \item \textbf{Continuous input} (squared residuals $r_t^2 = (y_t - \mu_t)^2$): ADWIN, KSWIN, PageHinkley. These methods track a running statistic of the input stream and alarm when it changes significantly.
    \item \textbf{Binary input} (error indicator $b_t = \mathbf{1}[|r_t| > \theta]$ where $\theta$ is the median absolute residual on the training data): DDM, EDDM, HDDM\_A, HDDM\_W. These methods are designed for binary error streams and alarm when the
    error rate increases.
\end{itemize}

PITMonitor receives PIT values; all baselines receive the appropriate residual-derived signal. This is the fairest comparison: each method gets the input it was designed for.

\paragraph{Evaluation protocol.}
We run $N = \expNtrials$ Monte Carlo trials per scenario, each using a distinct random seed for the data stream. For each trial we record whether an alarm fires, its index, and whether it occurred before or after the true drift onset, as well as distance from the true changepoint for PITMonitor only. We report:
\begin{itemize}
    \item \textbf{TPR}: fraction of trials with a true-positive alarm (alarm fired after drift onset).
    \item \textbf{FPR}: fraction of trials with a false alarm (alarm fired before drift onset).
    \item \textbf{Mean detection delay}: mean number of samples between the true drift onset and the alarm, over true-positive trials only.
\end{itemize}

\subsection{Results}

Table~\ref{tab:results} presents the full results. Figure~\ref{fig:detection_rates}
visualizes TPR and FPR per method and scenario, and Figure~\ref{fig:single_run}
shows a representative single-run monitoring trace for the GRA scenario.

\input{../experiment/out/table_results}


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../experiment/out/fig_detection_rates.png}
\caption{TPR and FPR across all detectors and drift scenarios. The red dashed line marks PITMonitor's false alarm guarantee $\alpha = \expAlpha$.}
\label{fig:detection_rates}
\end{figure}


\paragraph{Type I error control.}
PITMonitor achieves FPR of \resPMGRAFPR, within the nominal $\alpha = \expAlpha$ guarantee. This is consistent across all three scenarios, as expected since the pre-drift monitoring window is identical. ADWIN shows a consistent FPR of \resADGRAFPR, well above its $\delta = \expDelta$ parameter; this is expected because ADWIN's $\delta$ provides a per-comparison bound (Theorem~3.1 of \citet{bifet2007learning}), not an anytime-valid guarantee on the probability of ever alarming. Over a \expNstable-sample stable window with many subwindow comparisons, the cumulative false alarm probability substantially exceeds the per-step bound.

\paragraph{Global drift (GRA, GSG).}
PITMonitor achieves \resPMGRATPR{} TPR with \resPMGRAFPR{} FPR on both global drift scenarios, the highest power of any method with controlled false alarm rate. ADWIN achieves \resADGRATPR{} TPR with \resADGRAFPR{} FPR; its consistently elevated FPR is an artifact of ADWIN's per-step guarantee, not a code error (see Type I error discussion above). PITMonitor's detection delay is \resPMGRADelay{} samples on GRA and \resPMGSGDelay{} on GSG, compared to ADWIN's \resADGRADelay{} on both. DDM and HDDM\_A achieve \resDDMGRATPR{} and \resHDDMAGRATPR{} TPR respectively but with elevated FPR (\resDDMGRAFPR{} and \resHDDMAGRAFPR{}), while DDM's mean delay of \resDDMGRADelay{} samples (GRA) reflects its slower adaptation on the binary-thresholded input stream.

\paragraph{Local expanding drift (LEA).}
PITMonitor achieves \resPMLEATPR{} TPR on the LEA scenario, correctly maintaining FPR control but failing to detect the drift. ADWIN achieves \resADLEATPR{} TPR (its identical detection rate across all scenarios reflects the fact that ADWIN fires on every trial; what varies is only whether the alarm occurs before or after drift onset). This result reveals a meaningful limitation of PITMonitor in the current configuration: LEA drift begins on a subset of the five relevant features, producing a small initial perturbation to the Gaussian predictive distribution that the histogram e-value does not accumulate evidence for quickly enough within the \expNpost-sample post-drift window. ADWIN's squared-residual statistic appears more sensitive to partial distributional shifts than PITMonitor's PIT-based approach.

\paragraph{Detectors with collapsed FPR.}
KSWIN, PageHinkley, HDDM\_W, and EDDM exhibit FPR near 1, indicating they alarm on virtually every trial and do so predominantly during the pre-drift window. For KSWIN and PageHinkley this occurs because their default sensitivity parameters are not suited to monitoring squared residuals from a well-fitted Gaussian model, causing them to alarm during the burn-in period. HDDM\_W and EDDM similarly alarm aggressively on the binary-thresholded input stream, where the pre-drift error rate is approximately 50\% by construction (threshold is the median). These detectors would likely behave more reasonably with tuned hyperparameters; we report default-parameter results throughout to keep comparisons fair.

\paragraph{Changepoint localization.}
A unique property of PITMonitor is the Bayes-factor changepoint estimate available after each alarm. None of the \texttt{river} baselines provide an analogous estimate; they expose only a binary alarm flag. Figure~\ref{fig:cp_error} shows the distribution of absolute changepoint error $|\hat\tau - \tau|$ across true-positive trials. PITMonitor's mean absolute changepoint error is \resPMGRACPErr{} samples on GRA and \resPMGSGCPErr{} on GSG, demonstrating that the estimated changepoint closely tracks the true onset. This provides the practitioner with a starting point for diagnosing and correcting the drift.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../experiment/out/fig_cp_error_distribution.png}
\caption{Distribution of PITMonitor changepoint estimation error $|\hat\tau - \tau|$ across true-positive trials. The abrupt (GRA) scenario yields highly concentrated estimates; the gradual (GSG) scenario shows slightly larger errors due to the extended transition window.}
\label{fig:cp_error}
\end{figure}

%------------------------------------------------------------------------------
\section{Discussion}
%------------------------------------------------------------------------------

\paragraph{When to use PITMonitor.}
PITMonitor is designed for continuous monitoring of deployed probabilistic
models where:
\begin{itemize}
    \item False alarms have real costs (unnecessary retraining, alert fatigue, loss
    of trust)
    \item The monitoring horizon is indefinite or stopping is data-dependent
    \item \emph{Global} calibration drift, not localized feature degradation, is
    the concern
    \item Accurate changepoint localisation is valuable
\end{itemize}

For one-time calibration assessment (``is this model calibrated?''), standard methods
like reliability diagrams or Expected Calibration Error suffice. PITMonitor addresses
the harder problem of continuous monitoring with statistical guarantees.

\paragraph{Limitations.}

\textit{Local and partial drift.}
The LEA results show PITMonitor has low power when drift is initially confined to a subset of features. When only some input directions shift, the predictive Gaussian may change in mean or variance only slightly, producing weak signal in the PIT sequence. ADWIN on squared residuals can more directly detect a mean shift in prediction error regardless of how many features are responsible. Practitioners monitoring models where feature-level drift is anticipated may need to complement PITMonitor with feature-level monitors.

\textit{Detection delay vs.\ FPR control.}
PITMonitor's anytime-valid guarantee comes at the cost of later detection compared to ADWIN. The mean delay on GRA (76 samples) reflects the number of post-drift observations needed for the e-process to cross $1/\alpha = 20$. The most direct way to reduce delay is to accept a larger $\alpha$: setting $\alpha = 0.10$ lowers the threshold to 10, requiring less accumulated evidence before an alarm fires. Users should calibrate $\alpha$ against their tolerance for false alarms over the full expected deployment horizon rather than treating it as a fixed convention.

\textit{Exchangeability assumption.}
PITMonitor tests exchangeability of PITs. If pre-change PITs exhibit temporal
dependence (e.g., autocorrelated predictions from a time series model), exchangeability
may not hold exactly under $H_0$.

\textit{n\_bins sensitivity.}
The number of histogram bins $B$ controls the bias-variance tradeoff in the density
estimator. We use $B = 100$ for these experiments; empirically this provides good
adaptation speed with stable FPR. Smaller $B$ is more stable but slower to adapt;
larger $B$ adapts faster but at the cost of more variance in the estimated density.

\paragraph{Practical recommendations.}
\begin{itemize}
    \item Set $\alpha$ based on tolerance for false alarms over the deployment horizon.
    For safety-critical systems, $\alpha = 0.01$ may be appropriate; for exploratory
    monitoring, $\alpha = 0.10$ allows faster detection.
    \item Use $B = 100$ histogram bins as a default for large monitoring windows;
    reduce to $B = 10$--$20$ for smaller windows ($n_\text{monitor} < 500$).
    \item After an alarm, use the changepoint estimate to identify when drift began,
    then investigate root causes before retraining.
    \item For models where localized feature drift is anticipated, consider running
    PITMonitor in parallel with ADWIN on squared residuals. PITMonitor provides the changepoint localisation and anytime-valid FPR control; ADWIN provides faster detection of partial shifts, accepting a higher false alarm rate.
\end{itemize}

\paragraph{Comparison to \texttt{river} baselines.}
The experiments reveal a clear partitioning of methods. PITMonitor and ADWIN are the only detectors achieving meaningful TPR across global drift scenarios. Their tradeoff is qualitatively different: ADWIN detects roughly 3$\times$ faster but with substantially higher FPR (\resADGRAFPR{} vs.\ \resPMGRAFPR). Crucially, ADWIN's elevated FPR is expected given its per-step guarantee semantics: $\delta = \expDelta$ bounds the false alarm probability per comparison, not over the full stream, whereas PITMonitor's $\alpha = \expAlpha$ bounds the probability of \emph{ever} alarming. On local expanding drift ADWIN is dominant. The remaining methods either have collapsed FPR (KSWIN, PageHinkley, HDDM\_W, EDDM with default parameters) or elevated FPR (DDM at \resDDMGRAFPR, HDDM\_A at \resHDDMAGRAFPR), limiting their practical utility in this monitoring setting without careful tuning.

%------------------------------------------------------------------------------
\section{Related Work}
%------------------------------------------------------------------------------

\subsubsection*{Calibration Assessment}
Classical calibration metrics include Expected Calibration Error
\citep{naeini2015obtaining}, reliability diagrams \citep{degroot1983comparison}, and
proper scoring rules \citep{gneiting2007strictly}. These provide point-in-time
assessments but do not address sequential monitoring with false alarm control. PITs
have been used for forecast evaluation in econometrics \citep{diebold1998evaluating}
and weather prediction \citep{gneiting2014probabilistic}.

\subsubsection*{Distribution Shift Detection}
Methods for detecting covariate shift include two-sample tests
\citep{rabanser2019failing}, domain classifiers \citep{lipton2018detecting}, and
conformal approaches \citep{podkopaev2021distribution}. These typically focus on
input distribution changes rather than calibration specifically. Our work focuses on
the \emph{output} side: detecting when predicted probabilities no longer match outcome
frequencies.

\subsubsection*{Sequential Calibration Testing}
\citet{arnold2023sequentially} proposed e-values for testing forecast calibration,
focusing on whether PITs are uniform. Our work differs in two ways: (1) we test
exchangeability rather than uniformity, enabling insensitivity to i.i.d.\ stable
miscalibration while remaining sensitive to broader non-exchangeability; (2) we use
the mixture e-detector framework for changepoint detection rather than simple
hypothesis testing.

\subsubsection*{E-values and Anytime-Valid Inference}
The e-value framework has seen rapid development \citep{vovk2021values,
ramdas2023game, grunwald2024safe}. Applications include A/B testing
\citep{johari2022always}, clinical trials \citep{wassmer2016group}, and conformal
prediction \citep{vovk2005algorithmic}. The e-detector framework for changepoint
detection was introduced by \citet{shin2022detectors}, providing the theoretical
foundation for our mixture e-process.

\subsubsection*{Changepoint Detection}
Classical methods include CUSUM \citep{page1954continuous} and Shiryaev-Roberts
procedures \citep{shiryaev1963, pollak1985optimal}. These typically assume known
pre- and post-change distributions. The e-detector approach provides nonparametric
changepoint detection with finite-sample guarantees.

%------------------------------------------------------------------------------
\section{Conclusion}
%------------------------------------------------------------------------------

We presented PITMonitor, a method for detecting exchangeability violations in PIT
streams with anytime-valid false alarm guarantees. By testing exchangeability of
probability integral transforms using a mixture e-process, PITMonitor enables
continuous monitoring without inflating Type I error, regardless of when or why
monitoring stops.

Experiments on three FriedmanDrift scenarios demonstrate that PITMonitor matches or
exceeds the best competing method on global drift (GRA and GSG), achieving \resPMGRATPR{} TPR
with \resPMGRAFPR{} FPR and a unique changepoint localization capability. The results also surface
an honest limitation: PITMonitor has zero detection power on local expanding drift
(LEA) at the tested window sizes, while ADWIN detects this scenario with \resADLEATPR{} TPR
(though at \resADLEAFPR{} FPR).
Practitioners should therefore view PITMonitor as the right tool for global calibration
monitoring with strict FPR control, and consider complementing it with simpler
residual-based detectors when feature-level drift is anticipated.

Future work includes extensions to temporally dependent predictions, multivariate
outputs, and improving power on partial distributional shifts.

\paragraph{Code Availability.}
PITMonitor is available at \url{https://github.com/tristan-farran/pitmon}.

%------------------------------------------------------------------------------
% References
%------------------------------------------------------------------------------

\bibliographystyle{plainnat}

\bibliography{pitmon_references}
\end{document}

"""Publication-quality plots for the drift detection comparison.

Produces:
    fig_detection_rates.png      – grouped bar chart: TPR / FPR per method × scenario
    fig_delay_distributions.png  – violin + strip plots of detection delay
    fig_cp_error_distribution.png – histogram of PITMonitor changepoint estimation error
    fig_single_run_<scen>.png    – 4-panel single-run diagnostic per scenario
    table_results.tex            – LaTeX booktabs table for the paper
    experiment_macros.tex         – LaTeX \\newcommand macros for experiment params & results

All plot helpers accept a *save_dir* Path and save to that directory.
They also return the Figure so callers can embed figures programmatically.
"""

from __future__ import annotations

from pathlib import Path
from typing import Optional

import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
from matplotlib.lines import Line2D
import numpy as np

from detectors import ALL_DETECTOR_NAMES


# ─── Style ───────────────────────────────────────────────────────────

SCENARIO_LABELS: dict[str, str] = {
    "gra_tw0": "Abrupt (GRA)",
    "gsg_tw500": "Gradual (GSG)",
    "lea_tw0": "Local (LEA)",
}

# Short labels for compact table/axis use
SCENARIO_SHORT: dict[str, str] = {
    "gra_tw0": "GRA",
    "gsg_tw500": "GSG",
    "lea_tw0": "LEA",
}

METHOD_COLORS: dict[str, str] = {
    "PITMonitor": "#1b6ec2",
    "ADWIN": "#7c3aed",
    "KSWIN": "#0d9488",
    "PageHinkley": "#16a34a",
    "DDM": "#ca8a04",
    "EDDM": "#ea580c",
    "HDDM_A": "#db2777",
    "HDDM_W": "#9d174d",
}

# Hatching patterns to distinguish methods in greyscale printing
METHOD_HATCHES: dict[str, str] = {
    "PITMonitor": "",
    "ADWIN": "//",
    "KSWIN": "\\\\",
    "PageHinkley": "xx",
    "DDM": "..",
    "EDDM": "++",
    "HDDM_A": "--",
    "HDDM_W": "||",
}

_RC_OVERRIDES = {
    "font.family": "serif",
    "font.serif": ["CMU Serif", "Computer Modern Roman", "DejaVu Serif"],
    "mathtext.fontset": "cm",
    # Use ASCII hyphen-minus instead of Unicode minus (U+2212) to avoid missing
    # glyph warnings when the active font does not include that code point.
    # Applied globally at import time AND inside _apply_style() to ensure the
    # setting is in effect before any matplotlib rendering begins.
    "axes.unicode_minus": False,
    "font.size": 9,
    "axes.titlesize": 10,
    "axes.titleweight": "bold",
    "axes.labelsize": 9,
    "xtick.labelsize": 8,
    "ytick.labelsize": 8,
    "legend.fontsize": 7.5,
    "legend.framealpha": 0.85,
    "legend.edgecolor": "0.7",
    "figure.dpi": 200,
    "savefig.dpi": 300,
    "savefig.bbox": "tight",
    "axes.spines.top": False,
    "axes.spines.right": False,
    "axes.grid": True,
    "grid.alpha": 0.25,
    "grid.linewidth": 0.5,
    "lines.linewidth": 1.5,
    "patch.linewidth": 0.5,
}


# Apply globally at import time so the settings are in effect before any
# matplotlib code runs (e.g. log-scale tick formatters created at figure build).
plt.rcParams.update(_RC_OVERRIDES)


def _apply_style() -> None:
    """Apply shared rcParams to all subsequent matplotlib calls."""
    plt.rcParams.update(_RC_OVERRIDES)


def _fmt_pct(x: float) -> str:
    """Format a rate as a LaTeX-safe percentage string."""
    if np.isnan(x):
        return "—"
    return f"{x:.1%}".replace("%", r"\%")


def _fmt_delay(x: float) -> str:
    """Format a delay value."""
    if np.isnan(x):
        return "—"
    return f"{x:.0f}"


# ─── Figure 1: Detection rate comparison ────────────────────────────


def generate_experiment_macros(results: dict, save_dir: Path) -> str:
    r"""Generate a LaTeX file of \newcommand macros for experiment params and key results.

    This allows the paper to use macros like \expEpochs, \expNtrials, etc.
    so that all numbers stay in sync with the experiment automatically.

    Parameters
    ----------
    results : dict
    save_dir : Path

    Returns
    -------
    str
        Complete LaTeX macro file source.
    """
    cfg = results["config"]
    lines = [
        "% Auto-generated by plots.py — do not edit manually.",
        "% Experiment configuration macros",
        rf"\newcommand{{\expSeed}}{{{cfg['seed']}}}",
        rf"\newcommand{{\expEpochs}}{{{cfg['epochs']:,}}}".replace(",", "{,}"),
        rf"\newcommand{{\expLr}}{{3 \times 10^{{-4}}}}",
        rf"\newcommand{{\expNtrain}}{{{cfg['n_train']:,}}}".replace(",", "{,}"),
        rf"\newcommand{{\expNstable}}{{{cfg['n_stable']:,}}}".replace(",", "{,}"),
        rf"\newcommand{{\expNpost}}{{{cfg['n_post']:,}}}".replace(",", "{,}"),
        rf"\newcommand{{\expAlpha}}{{{cfg['alpha']}}}",
        rf"\newcommand{{\expNbins}}{{{cfg['n_bins']}}}",
        rf"\newcommand{{\expNtrials}}{{{cfg['n_trials']:,}}}".replace(",", "{,}"),
        rf"\newcommand{{\expBinaryThreshold}}{{{cfg['binary_threshold']:.4f}}}",
        "",
        "% Per-scenario result macros (PITMonitor)",
    ]

    res = results["results"]
    for scen_key, short in SCENARIO_SHORT.items():
        tag = short  # e.g. GRA, GSG, LEA
        pm = res.get(scen_key, {}).get("PITMonitor", {})
        if not pm:
            continue
        tpr = pm.get("tpr", float("nan"))
        fpr = pm.get("fpr", float("nan"))
        delay = pm.get("mean_delay", float("nan"))
        cp_err = pm.get("mean_cp_error", float("nan"))

        lines.append(f"% {tag}")
        lines.append(rf"\newcommand{{\resPM{tag}TPR}}{{{tpr:.1%}}}".replace("%", r"\%"))
        lines.append(rf"\newcommand{{\resPM{tag}FPR}}{{{fpr:.1%}}}".replace("%", r"\%"))
        if not np.isnan(delay):
            lines.append(rf"\newcommand{{\resPM{tag}Delay}}{{{delay:.0f}}}")
        if not np.isnan(cp_err):
            lines.append(rf"\newcommand{{\resPM{tag}CPErr}}{{{cp_err:.1f}}}")

    # ADWIN results
    lines.append("")
    lines.append("% Per-scenario result macros (ADWIN)")
    for scen_key, short in SCENARIO_SHORT.items():
        tag = short
        ad = res.get(scen_key, {}).get("ADWIN", {})
        if not ad:
            continue
        tpr = ad.get("tpr", float("nan"))
        fpr = ad.get("fpr", float("nan"))
        delay = ad.get("mean_delay", float("nan"))
        lines.append(f"% {tag}")
        lines.append(rf"\newcommand{{\resAD{tag}TPR}}{{{tpr:.1%}}}".replace("%", r"\%"))
        lines.append(rf"\newcommand{{\resAD{tag}FPR}}{{{fpr:.1%}}}".replace("%", r"\%"))
        if not np.isnan(delay):
            lines.append(rf"\newcommand{{\resAD{tag}Delay}}{{{delay:.0f}}}")

    # DDM and HDDM_A (mentioned in paper text)
    lines.append("")
    lines.append("% Other detectors mentioned in text")
    for det_name, prefix in [("DDM", "DDM"), ("HDDM_A", "HDDMA")]:
        for scen_key, short in SCENARIO_SHORT.items():
            d = res.get(scen_key, {}).get(det_name, {})
            if not d:
                continue
            tpr = d.get("tpr", float("nan"))
            fpr = d.get("fpr", float("nan"))
            delay = d.get("mean_delay", float("nan"))
            tag = short
            lines.append(
                rf"\newcommand{{\res{prefix}{tag}TPR}}{{{tpr:.1%}}}".replace("%", r"\%")
            )
            lines.append(
                rf"\newcommand{{\res{prefix}{tag}FPR}}{{{fpr:.1%}}}".replace("%", r"\%")
            )
            if not np.isnan(delay):
                lines.append(rf"\newcommand{{\res{prefix}{tag}Delay}}{{{delay:.0f}}}")

    latex_src = "\n".join(lines) + "\n"
    out = save_dir / "experiment_macros.tex"
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(latex_src)
    print(f"  Saved {out}")
    return latex_src


def plot_detection_rates(results: dict, save_dir: Path) -> plt.Figure:
    """Grouped bar chart for TPR and FPR per method and scenario.

    Parameters
    ----------
    results : dict
        Output from ``run_experiment`` (loaded from JSON).
    save_dir : Path

    Returns
    -------
    plt.Figure
    """
    _apply_style()
    scenarios = [k for k in results["results"] if k in SCENARIO_LABELS]
    if not scenarios:
        scenarios = list(results["results"].keys())

    alpha = results["config"]["alpha"]
    n_scenarios = len(scenarios)

    fig, axes = plt.subplots(
        n_scenarios,
        2,
        figsize=(7.0, 2.7 * n_scenarios + 0.6),
        gridspec_kw={"wspace": 0.35, "hspace": 0.55},
    )
    fig.subplots_adjust(top=0.93)
    if n_scenarios == 1:
        axes = np.array([axes])

    for i, scen in enumerate(scenarios):
        scen_label = SCENARIO_LABELS.get(scen, scen)
        methods, tprs, fprs, tpr_err, fpr_err, colors = [], [], [], [], [], []

        for method in ALL_DETECTOR_NAMES:
            s = results["results"][scen].get(method, {})
            tpr = s.get("tpr", 0.0)
            fpr = s.get("fpr", 0.0)
            methods.append(method)
            tprs.append(tpr)
            fprs.append(fpr)

            ci = s.get("tpr_ci", (tpr, tpr))
            tpr_err.append([tpr - ci[0], ci[1] - tpr])
            ci = s.get("fpr_ci", (fpr, fpr))
            fpr_err.append([fpr - ci[0], ci[1] - fpr])
            colors.append(METHOD_COLORS.get(method, "#999999"))

        y = np.arange(len(methods))
        tpr_err_arr = np.abs(np.array(tpr_err).T)
        fpr_err_arr = np.abs(np.array(fpr_err).T)

        # TPR panel
        ax = axes[i, 0]
        bars = ax.barh(
            y,
            tprs,
            xerr=tpr_err_arr,
            color=colors,
            alpha=0.88,
            edgecolor="white",
            linewidth=0.5,
            error_kw={"lw": 0.8, "capsize": 2, "capthick": 0.8},
        )
        for bar, method in zip(bars, methods):
            bar.set_hatch(METHOD_HATCHES.get(method, ""))
        ax.set_yticks(y)
        ax.set_yticklabels(methods, fontsize=8)
        ax.set_xlim(0, 1.08)
        ax.set_title(f"{scen_label} — TPR")
        ax.xaxis.set_major_formatter(mticker.PercentFormatter(1.0, 0))

        # FPR panel — capped at FPR_CAP; bars exceeding it are annotated
        FPR_CAP = 0.15
        ax = axes[i, 1]
        fprs_display = [min(f, FPR_CAP) for f in fprs]
        # Suppress error bars on clipped bars (they'd extend beyond cap)
        fpr_err_display = [
            e if f < FPR_CAP * 0.98 else [0.0, 0.0]
            for f, e in zip(fprs, fpr_err)
        ]
        fpr_err_arr_display = np.abs(np.array(fpr_err_display).T)
        bars = ax.barh(
            y,
            fprs_display,
            xerr=fpr_err_arr_display,
            color=colors,
            alpha=0.88,
            edgecolor="white",
            linewidth=0.5,
            error_kw={"lw": 0.8, "capsize": 2, "capthick": 0.8},
        )
        for bar, method in zip(bars, methods):
            bar.set_hatch(METHOD_HATCHES.get(method, ""))
        # Annotate clipped bars with actual value
        for bar, fpr in zip(bars, fprs):
            if fpr >= FPR_CAP * 0.98:
                ax.text(
                    FPR_CAP * 1.02,
                    bar.get_y() + bar.get_height() / 2,
                    f"{fpr:.0%}",
                    va="center",
                    ha="left",
                    fontsize=6.5,
                    color="0.3",
                )
        ax.axvline(
            alpha,
            color="crimson",
            ls="--",
            lw=1.2,
            alpha=0.8,
            label=f"α = {alpha}",
        )
        ax.set_yticks(y)
        ax.set_yticklabels([])
        ax.set_xlim(0, FPR_CAP * 1.40)
        ax.set_title(f"{scen_label} — FPR")
        ax.xaxis.set_major_formatter(mticker.PercentFormatter(1.0, 0))
        if i == 0:
            ax.legend(fontsize=7, loc="lower right")

    fig.suptitle(
        "Drift Detection: True and False Positive Rates",
        fontsize=11,
        fontweight="bold",
    )

    out = save_dir / "fig_detection_rates.png"
    fig.savefig(out, bbox_inches="tight")
    plt.close(fig)
    print(f"  Saved {out}")
    return fig


# ─── Figure 2: Detection delay distributions ────────────────────────


def plot_delay_distributions(results: dict, save_dir: Path) -> plt.Figure:
    """Violin + jittered strip plots of detection delay (true positives).

    All detectors appear on the x-axis regardless of whether they fired.
    Methods with zero detections show an annotation.  Methods with near-zero
    variance (e.g. ADWIN firing deterministically) show a jittered strip
    with a median marker instead of a violin.

    Parameters
    ----------
    results : dict
    save_dir : Path

    Returns
    -------
    plt.Figure
    """
    _apply_style()
    scenarios = [k for k in results["results"] if k in SCENARIO_LABELS]
    if not scenarios:
        scenarios = list(results["results"].keys())

    n_scenarios = len(scenarios)
    fig, axes = plt.subplots(
        1,
        n_scenarios,
        figsize=(4.5 * n_scenarios, 5.5),
        sharey=False,
    )
    fig.subplots_adjust(top=0.88, bottom=0.18, wspace=0.3)
    if n_scenarios == 1:
        axes = [axes]

    for ax, scen in zip(axes, scenarios):
        scen_label = SCENARIO_LABELS.get(scen, scen)
        all_methods = list(ALL_DETECTOR_NAMES)
        positions = list(range(1, len(all_methods) + 1))
        method_colors = [METHOD_COLORS.get(m, "#999999") for m in all_methods]

        # Collect delay data per method
        method_delays = {}
        for method in all_methods:
            delays = results["results"][scen].get(method, {}).get("delays", [])
            method_delays[method] = delays

        # Classify each method
        violin_data, violin_pos, violin_colors = [], [], []
        strip_data, strip_pos, strip_colors = [], [], []
        no_detection_pos, no_detection_labels, no_detection_colors = [], [], []

        for method, pos, color in zip(all_methods, positions, method_colors):
            delays = method_delays[method]
            if len(delays) == 0:
                no_detection_pos.append(pos)
                no_detection_labels.append(method)
                no_detection_colors.append(color)
            elif len(delays) <= 2 or np.std(delays) < 1.0:
                # Near-zero variance or too few points for a violin
                strip_data.append(np.array(delays))
                strip_pos.append(pos)
                strip_colors.append(color)
            else:
                violin_data.append(delays)
                violin_pos.append(pos)
                violin_colors.append(color)

        # Draw violins for methods with enough variance
        if violin_data:
            parts = ax.violinplot(
                violin_data,
                positions=violin_pos,
                showmedians=False,
                showextrema=False,
            )
            for body, color in zip(parts["bodies"], violin_colors):
                body.set_facecolor(color)
                body.set_alpha(0.3)
                body.set_edgecolor(color)
                body.set_linewidth(0.8)

            # Overlay box plot (thin)
            bp = ax.boxplot(
                violin_data,
                positions=violin_pos,
                patch_artist=True,
                widths=0.20,
                showfliers=False,
                medianprops={"color": "white", "linewidth": 1.6},
                whiskerprops={"linewidth": 1.0, "color": "0.4"},
                capprops={"linewidth": 1.0, "color": "0.4"},
                boxprops={"linewidth": 0.8},
            )
            for patch, color in zip(bp["boxes"], violin_colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.85)

        # Draw jittered strip + median for low-variance methods
        for delays, pos, color in zip(strip_data, strip_pos, strip_colors):
            jitter = np.random.default_rng(42).normal(0, 0.06, size=len(delays))
            ax.scatter(
                pos + jitter,
                delays,
                s=8,
                alpha=0.4,
                color=color,
                zorder=3,
                rasterized=True,
            )
            med = np.median(delays)
            ax.plot(
                [pos - 0.15, pos + 0.15],
                [med, med],
                color=color,
                lw=2.5,
                solid_capstyle="round",
                zorder=4,
            )

        # Tight y-limits based on all available data; must be set before
        # placing no-detection annotations so we know the axis range.
        all_delays = [d for m in all_methods for d in method_delays[m]]
        if all_delays:
            all_vals = np.array(all_delays)
            lo = max(0, float(np.percentile(all_vals, 1)))
            hi = float(np.percentile(all_vals, 99))
            pad = max((hi - lo) * 0.08, 5)
            ax.set_ylim(lo - pad, hi + pad)

        # Ensure all method positions are visible regardless of data presence.
        ax.set_xlim(0.5, len(all_methods) + 0.5)

        # Annotate methods with zero true-positive detections with a marker
        # at the bottom of the axis so they are still visible at their position.
        if no_detection_pos:
            y_lo, y_hi = ax.get_ylim()
            marker_y = y_lo + (y_hi - y_lo) * 0.04
            for pos, color in zip(no_detection_pos, no_detection_colors):
                ax.plot(
                    pos,
                    marker_y,
                    marker="x",
                    markersize=7,
                    color=color,
                    markeredgewidth=1.5,
                    zorder=5,
                    clip_on=False,
                )

        ax.set_xticks(positions)
        ax.set_xticklabels(all_methods, rotation=40, ha="right", fontsize=7.5)
        ax.set_title(scen_label)

    axes[0].set_ylabel("Detection delay (samples)")
    fig.suptitle(
        "Detection Delay Distributions (True Positives Only)",
        fontsize=11,
        fontweight="bold",
    )

    out = save_dir / "fig_delay_distributions.png"
    fig.savefig(out, bbox_inches="tight")
    plt.close(fig)
    print(f"  Saved {out}")
    return fig


# ─── LaTeX summary table ────────────────────────────────────────────


def generate_latex_table(results: dict, save_dir: Path) -> str:
    r"""Generate a LaTeX booktabs table with all summary statistics.

    Columns per scenario: TPR, FPR, Mean Delay.  Changepoint error is
    reported separately (see ``plot_cp_error_distribution``).

    Parameters
    ----------
    results : dict
    save_dir : Path

    Returns
    -------
    str
        Complete LaTeX table source (ready to \\input{} in a paper).
    """
    scenarios = [k for k in results["results"] if k in SCENARIO_LABELS]
    if not scenarios:
        scenarios = list(results["results"].keys())

    n_scen = len(scenarios)
    alpha = results["config"]["alpha"]
    n_trials = results["config"]["n_trials"]

    # Build column spec: Method | (TPR FPR Delay) × n_scenarios
    col_spec = "l" + " ccc" * n_scen

    lines = []
    lines.append(r"\begin{table}[t]")
    lines.append(r"\centering")
    lines.append(r"\small")
    lines.append(
        r"\caption{Drift detection results on FriedmanDrift "
        rf"(\expNtrials{{}} trials, $\alpha=\expAlpha$). "
        r"Mean delay is in samples. "
        r"Best TPR per scenario is \textbf{bolded}; "
        r"FPR exceeding $\alpha$ is \underline{underlined}.}"
    )
    lines.append(r"\label{tab:results}")
    lines.append(rf"\begin{{tabular}}{{{col_spec}}}")
    lines.append(r"\toprule")

    # Header row 1: scenario names spanning 3 columns each
    header1_parts = [r"\multicolumn{1}{l}{}"]
    for scen in scenarios:
        label = SCENARIO_SHORT.get(scen, scen)
        header1_parts.append(rf"\multicolumn{{3}}{{c}}{{{label}}}")
    lines.append(" & ".join(header1_parts) + r" \\")

    # Cmidrules under each scenario group
    cmidrules = []
    for i, _scen in enumerate(scenarios):
        start = 2 + i * 3
        end = start + 2
        cmidrules.append(rf"\cmidrule(lr){{{start}-{end}}}")
    lines.append(" ".join(cmidrules))

    # Header row 2: metric names
    header2_parts = ["Method"]
    for _scen in scenarios:
        header2_parts.extend(["TPR", "FPR", "Delay"])
    lines.append(" & ".join(header2_parts) + r" \\")
    lines.append(r"\midrule")

    # Find best TPR per scenario for bolding
    best_tpr = {}
    for scen in scenarios:
        tprs = []
        for method in ALL_DETECTOR_NAMES:
            s = results["results"][scen].get(method, {})
            tprs.append(s.get("tpr", 0.0))
        best_tpr[scen] = max(tprs)

    # Data rows
    for method in ALL_DETECTOR_NAMES:
        row_parts = [method.replace("_", r"\_")]

        for scen in scenarios:
            s = results["results"][scen].get(method, {})
            tpr = s.get("tpr", float("nan"))
            fpr = s.get("fpr", float("nan"))
            delay = s.get("mean_delay", float("nan"))

            # Format TPR (bold if best)
            tpr_str = _fmt_pct(tpr)
            if not np.isnan(tpr) and abs(tpr - best_tpr[scen]) < 1e-6:
                tpr_str = rf"\textbf{{{tpr_str}}}"

            # Format FPR (underline if > alpha)
            fpr_str = _fmt_pct(fpr)
            if not np.isnan(fpr) and fpr > alpha:
                fpr_str = rf"\underline{{{fpr_str}}}"

            delay_str = _fmt_delay(delay)

            row_parts.extend([tpr_str, fpr_str, delay_str])

        lines.append(" & ".join(row_parts) + r" \\")

    lines.append(r"\bottomrule")
    lines.append(r"\end{tabular}")
    lines.append(r"\end{table}")

    latex_src = "\n".join(lines)

    out = save_dir / "table_results.tex"
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(latex_src)
    print(f"  Saved {out}")
    return latex_src


# ─── Figure: Changepoint estimation error distribution ────────────


def plot_cp_error_distribution(results: dict, save_dir: Path) -> Optional[plt.Figure]:
    """Histogram of PITMonitor changepoint estimation error across scenarios.

    Shows the distribution of |τ̂ − τ| for all true-positive trials where
    PITMonitor provides a changepoint estimate.  Scenarios with zero
    true positives (e.g. LEA) are omitted.

    Parameters
    ----------
    results : dict
    save_dir : Path

    Returns
    -------
    plt.Figure or None
        None if no changepoint data is available.
    """
    _apply_style()
    scenarios = [k for k in results["results"] if k in SCENARIO_LABELS]
    if not scenarios:
        scenarios = list(results["results"].keys())

    # Collect cp_errors per scenario
    plot_data = []
    for scen in scenarios:
        pm = results["results"][scen].get("PITMonitor", {})
        cp_errors = pm.get("cp_errors", [])
        if cp_errors:
            label = SCENARIO_LABELS.get(scen, scen)
            mean_err = pm.get("mean_cp_error", np.mean(cp_errors))
            median_err = pm.get("median_cp_error", np.median(cp_errors))
            plot_data.append(
                {
                    "label": label,
                    "short": SCENARIO_SHORT.get(scen, scen),
                    "errors": np.array(cp_errors),
                    "mean": mean_err,
                    "median": median_err,
                }
            )

    if not plot_data:
        print("  Skipped fig_cp_error_distribution.png (no CP data)")
        return None

    n_panels = len(plot_data)
    fig, axes = plt.subplots(
        1,
        n_panels,
        figsize=(4.5 * n_panels, 4.2),
        squeeze=False,
    )
    fig.subplots_adjust(top=0.82, wspace=0.35)
    axes = axes[0]

    color = METHOD_COLORS["PITMonitor"]

    for ax, data in zip(axes, plot_data):
        errors = data["errors"]

        # Choose bin edges: integer-width bins since errors are in samples
        max_err = int(np.ceil(np.percentile(errors, 99.5))) + 1
        bins = np.arange(-0.5, max_err + 1.5, 1)

        ax.hist(
            errors,
            bins=bins,
            color=color,
            alpha=0.7,
            edgecolor="white",
            linewidth=0.4,
            density=False,
        )

        # Mark mean and median
        ax.axvline(
            data["mean"],
            color="crimson",
            ls="--",
            lw=1.3,
            label=f"Mean = {data['mean']:.1f}",
        )
        ax.axvline(
            data["median"],
            color="0.3",
            ls=":",
            lw=1.3,
            label=f"Median = {data['median']:.1f}",
        )

        ax.set_title(data["label"])
        ax.set_xlabel("Changepoint error $|\\hat{\\tau} - \\tau|$ (samples)")
        ax.set_ylabel("Count")
        ax.legend(fontsize=7.5)
        ax.set_xlim(-0.5, max_err + 0.5)

    fig.suptitle(
        "PITMonitor Changepoint Estimation Error",
        fontsize=11,
        fontweight="bold",
    )

    out = save_dir / "fig_cp_error_distribution.png"
    fig.savefig(out, bbox_inches="tight")
    plt.close(fig)
    print(f"  Saved {out}")
    return fig


# ─── Figure 4: Single-run panels ─────────────────────────────────────


def plot_single_run_panels(
    artifacts: dict,
    save_path: Optional[Path] = None,
) -> plt.Figure:
    """Four-panel diagnostic for a single monitoring run.

    Panels:
        Top-left  – Raw predictions vs actual values with alarm markers.
        Top-right – PIT stream with rolling mean and shift indicator.
        Bottom-left – E-process on a log scale with threshold and changepoint.
        Bottom-right – Pre-shift vs post-shift PIT histograms.

    Parameters
    ----------
    artifacts : dict
        Dictionary returned by ``collect_single_run``.
    save_path : Path or None

    Returns
    -------
    plt.Figure
    """
    _apply_style()

    true_shift_point = int(artifacts["true_shift_point"])
    y_all = np.asarray(artifacts["true_labels"], dtype=float)
    preds = np.asarray(artifacts["predictions"], dtype=float)
    pits = np.asarray(artifacts["pits"], dtype=float)
    evidence = np.asarray(artifacts["evidence_trace"], dtype=float)
    times = np.arange(1, len(y_all) + 1)

    # Determine scenario label for title
    scen_key = artifacts.get("scenario_key", "")
    scen_label = SCENARIO_LABELS.get(scen_key, scen_key)

    fig, axes = plt.subplots(
        2,
        2,
        figsize=(8.5, 6.4),
    )
    fig.subplots_adjust(top=0.90, hspace=0.45, wspace=0.35)
    fig.suptitle(
        f"PITMonitor Single Run — {scen_label}",
        fontsize=11,
        fontweight="bold",
    )

    # Colour definitions for consistency
    c_pre = "#4a90d9"
    c_post = "#d94a4a"
    c_alarm = "#e8920d"
    c_shift = "#c41e3a"
    c_cp = "#2e8b57"

    # ── Top-left: Predictions vs Reality ────────────────────────────
    ax = axes[0, 0]
    mask_pre = times < true_shift_point
    mask_post = ~mask_pre

    ax.scatter(
        times[mask_pre],
        y_all[mask_pre],
        s=3,
        alpha=0.25,
        c=c_pre,
        rasterized=True,
    )
    ax.scatter(
        times[mask_post],
        y_all[mask_post],
        s=3,
        alpha=0.25,
        c=c_post,
        rasterized=True,
    )
    ax.scatter(
        times,
        preds,
        s=2,
        alpha=0.20,
        c="0.3",
        rasterized=True,
    )
    ax.axvline(
        true_shift_point,
        color=c_shift,
        ls=":",
        lw=1.2,
        alpha=0.8,
    )
    if artifacts["alarm_fired"] and artifacts["alarm_time"] is not None:
        ax.axvline(
            int(artifacts["alarm_time"]),
            color=c_alarm,
            ls="--",
            lw=1.2,
        )
    legend_handles = [
        Line2D([0], [0], marker="o", ls="", color=c_pre, ms=3, label="Actual (pre)"),
        Line2D([0], [0], marker="o", ls="", color=c_post, ms=3, label="Actual (post)"),
        Line2D([0], [0], marker="o", ls="", color="0.3", ms=3, label="Predicted"),
        Line2D([0], [0], color=c_shift, ls=":", lw=1.2, label="True shift"),
    ]
    if artifacts["alarm_fired"] and artifacts["alarm_time"] is not None:
        legend_handles.append(
            Line2D(
                [0],
                [0],
                color=c_alarm,
                ls="--",
                lw=1.2,
                label=f"Alarm (t={artifacts['alarm_time']})",
            )
        )
    ax.legend(handles=legend_handles, fontsize=6.5, loc="upper right", ncol=2)
    ax.set(xlabel="Sample", ylabel="Target value", title="(a) Predictions vs reality")

    # ── Top-right: PIT stream ────────────────────────────────────────
    ax = axes[0, 1]
    point_colors = np.where(times < true_shift_point, c_pre, c_post)
    ax.scatter(times, pits, s=2, alpha=0.30, c=point_colors, rasterized=True)

    if len(pits) >= 50:
        w = 50
        rolling = np.convolve(pits, np.ones(w) / w, mode="valid")
        ax.plot(
            np.arange(w, len(pits) + 1),
            rolling,
            color="0.15",
            lw=1.0,
            alpha=0.8,
        )
    ax.axhline(0.5, color="0.5", ls="--", lw=0.8, alpha=0.6)
    ax.axvline(true_shift_point, color=c_shift, ls=":", lw=1.2, alpha=0.8)
    if artifacts["alarm_fired"] and artifacts["alarm_time"] is not None:
        ax.axvline(int(artifacts["alarm_time"]), color=c_alarm, ls="--", lw=1.2)

    leg = [
        Line2D([0], [0], marker="o", ls="", color=c_pre, ms=3, label="Pre-shift"),
        Line2D([0], [0], marker="o", ls="", color=c_post, ms=3, label="Post-shift"),
        Line2D([0], [0], color="0.15", lw=1.0, label="Rolling mean"),
        Line2D([0], [0], color="0.5", ls="--", lw=0.8, label="Ref (0.5)"),
        Line2D([0], [0], color=c_shift, ls=":", lw=1.2, label="True shift"),
    ]
    ax.legend(handles=leg, fontsize=6.5, loc="upper right", ncol=2)
    ax.set(xlabel="Sample", ylabel="PIT", title="(b) PIT stream", ylim=(-0.05, 1.05))

    # ── Bottom-left: E-process ───────────────────────────────────────
    ax = axes[1, 0]
    ax.semilogy(
        times,
        np.maximum(evidence, 1e-10),
        color=c_pre,
        lw=1.3,
    )
    threshold = 1.0 / float(artifacts["monitor_alpha"])
    ax.axhline(
        threshold,
        color=c_shift,
        ls="--",
        lw=1.2,
        label=f"Threshold (1/α = {threshold:.0f})",
    )
    ax.axvline(
        true_shift_point,
        color=c_shift,
        ls=":",
        lw=1.2,
        alpha=0.8,
        label="True shift",
    )
    if artifacts.get("changepoint") is not None:
        ax.axvline(
            int(artifacts["changepoint"]),
            color=c_cp,
            ls="--",
            lw=1.2,
            alpha=0.8,
            label=f"CP est. (t~{artifacts['changepoint']})",
        )
    if artifacts["alarm_fired"] and artifacts["alarm_time"] is not None:
        ax.axvline(
            int(artifacts["alarm_time"]),
            color=c_alarm,
            ls="--",
            lw=1.2,
            label=f"Alarm (t={artifacts['alarm_time']})",
        )
    ax.legend(fontsize=6.5, loc="upper left")
    ax.set(xlabel="Sample", ylabel="Evidence (log scale)", title="(c) E-process")

    # ── Bottom-right: PIT histograms ─────────────────────────────────
    ax = axes[1, 1]
    bins = np.linspace(0, 1, 21)
    pre_pits = pits[: true_shift_point - 1]
    post_pits = pits[true_shift_point - 1 :]
    if len(pre_pits) > 0:
        ax.hist(
            pre_pits,
            bins=bins,
            density=True,
            alpha=0.50,
            color=c_pre,
            edgecolor="white",
            linewidth=0.4,
            label=f"Pre-shift (n={len(pre_pits)})",
        )
    if len(post_pits) > 0:
        ax.hist(
            post_pits,
            bins=bins,
            density=True,
            alpha=0.50,
            color=c_post,
            edgecolor="white",
            linewidth=0.4,
            label=f"Post-shift (n={len(post_pits)})",
        )
    ax.axhline(1.0, color="0.3", ls="--", lw=1.0, label="U[0,1] reference")
    ax.legend(fontsize=6.5)
    ax.set(
        xlabel="PIT",
        ylabel="Density",
        title="(d) PIT distributions",
        xlim=(0, 1),
    )

    if save_path is not None:
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, bbox_inches="tight")
        print(f"  Saved {save_path}")
    plt.close(fig)
    return fig


# ─── Master plot function ────────────────────────────────────────────


def make_all_plots(results: dict, save_dir: Path) -> None:
    """Generate all publication figures and the LaTeX table from saved results.

    Parameters
    ----------
    results : dict
        Full output of ``run_experiment`` / ``load_results``.
    save_dir : Path
        Output directory; created if absent.
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    print("\nGenerating plots:")
    plot_detection_rates(results, save_dir)
    plot_delay_distributions(results, save_dir)
    plot_cp_error_distribution(results, save_dir)

    # LaTeX table (replaces the old PNG summary table)
    generate_latex_table(results, save_dir)

    # LaTeX macros for experiment parameters and key results
    generate_experiment_macros(results, save_dir)

    # Single-run panels
    for scenario_key, artifacts in results.get("single_runs", {}).items():
        out_path = save_dir / f"fig_single_run_{scenario_key}.png"
        plot_single_run_panels(artifacts, save_path=out_path)

    print("Done.")
